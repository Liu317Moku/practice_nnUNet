{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941bf0b2-bd0d-4ceb-a15f-e0315f2be862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnU-Net 資料夾已建立在 practice_nnunet/ 下\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 設定 nnU-Net 資料夾路徑\n",
    "base_dir = \"practice_nnunet\"\n",
    "nnUNet_raw = os.path.join(base_dir, \"nnUNet_raw\")\n",
    "nnUNet_preprocessed = os.path.join(base_dir, \"nnUNet_preprocessed\")\n",
    "nnUNet_results = os.path.join(base_dir, \"nnUNet_results\")\n",
    "\n",
    "# 建立資料夾\n",
    "os.makedirs(nnUNet_raw, exist_ok=True)\n",
    "os.makedirs(nnUNet_preprocessed, exist_ok=True)\n",
    "os.makedirs(nnUNet_results, exist_ok=True)\n",
    "\n",
    "# 設定環境變數\n",
    "os.environ[\"nnUNet_raw\"] = nnUNet_raw\n",
    "os.environ[\"nnUNet_preprocessed\"] = nnUNet_preprocessed\n",
    "os.environ[\"nnUNet_results\"] = nnUNet_results\n",
    "\n",
    "print(\"nnU-Net 資料夾已建立在 practice_nnunet/ 下\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d8e0cb-854c-409c-ad4b-5fe1e2956539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# -------------------------------\n",
    "# nnU-Net 資料夾\n",
    "# -------------------------------\n",
    "task_dir = \"/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw/Dataset001\" #改成自己的帳號名稱放置路徑中\n",
    "imagesTr_dir = os.path.join(task_dir, \"imagesTr\")\n",
    "labelsTr_dir = os.path.join(task_dir, \"labelsTr\")\n",
    "imagesTs_dir = os.path.join(task_dir, \"imagesTs\")\n",
    "os.makedirs(imagesTr_dir, exist_ok=True)\n",
    "os.makedirs(labelsTr_dir, exist_ok=True)\n",
    "os.makedirs(imagesTs_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 原始影像與 mask 資料夾\n",
    "# -------------------------------\n",
    "src_images = \"/home/sandy0317/Public/train/med-ddpm-1/image\" #改成自己的帳號名稱放置路徑中\n",
    "src_masks = \"/home/sandy0317/Public/train/med-ddpm-1/mask\" #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "# -------------------------------\n",
    "# 找出所有影像檔\n",
    "# -------------------------------\n",
    "image_paths = sorted(glob.glob(os.path.join(src_images, \"*\")))\n",
    "mask_paths = sorted(glob.glob(os.path.join(src_masks, \"*\")))\n",
    "\n",
    "# -------------------------------\n",
    "# 檢查數量一致\n",
    "# -------------------------------\n",
    "assert len(image_paths) == len(mask_paths), f\"影像數量({len(image_paths)})與標註數量({len(mask_paths)})不一致！\"\n",
    "\n",
    "# -------------------------------\n",
    "# 複製並重新命名\n",
    "# -------------------------------\n",
    "for idx, (img_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
    "    case_id = f\"seg_{idx:04d}\"\n",
    "\n",
    "    # nnU-Net 格式檔名\n",
    "    new_img_name = f\"{case_id}_0000.nii.gz\"  # 影像加 _0000\n",
    "    new_mask_name = f\"{case_id}.nii.gz\"      # mask 保留 case_id\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(imagesTr_dir, new_img_name))\n",
    "    shutil.copy(mask_path, os.path.join(labelsTr_dir, new_mask_name))\n",
    "\n",
    "print(f\"✅ 已完成轉換並複製 {len(image_paths)} 筆資料到 nnU-Net 格式資料夾：{task_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9a2dd9-7f9c-400b-b22d-d0824594c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 標籤檔已修正完成 (2→1)\n",
      "✅ dataset.json 已生成，背景標籤正確\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnunet/nnUNet_results\n",
      "🚀 開始 nnU-Net 資料集預處理...\n",
      "Fingerprint extraction...\n",
      "Dataset001\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:59<00:00, 16.87it/s]\n",
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Dropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [127. 128. 128.], 3d_lowres: [127, 128, 128]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 199, 'patch_size': (np.int64(128), np.int64(128)), 'median_image_size_in_voxels': array([128., 128.]), 'spacing': array([1.5, 1.5]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(128), np.int64(128), np.int64(128)), 'median_image_size_in_voxels': array([127., 128., 128.]), 'spacing': array([1.5, 1.5, 1.5]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n",
      "\n",
      "Plans were saved to /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed/Dataset001/nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset001\n",
      "Configuration: 2d...\n",
      "100%|███████████████████████████████████████| 1000/1000 [14:28<00:00,  1.15it/s]\n",
      "Configuration: 3d_fullres...\n",
      "100%|███████████████████████████████████████| 1000/1000 [22:55<00:00,  1.38s/it]\n",
      "Configuration: 3d_lowres...\n",
      "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset001. Skipping.\n",
      "✅ nnU-Net 資料集預處理完成\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# nnU-Net Dataset 前置流程 (Jupyter Notebook)\n",
    "# ===============================\n",
    "#執行前請先至Terminal確認自己的nnU-Net可執行檔的位置，如語法which nnUNetv2_plan_and_preprocess\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ 原始資料集路徑\n",
    "# -------------------------------\n",
    "task_dir = \"/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw/Dataset001\" #改成自己的帳號名稱放置路徑中\n",
    "imagesTr_dir = os.path.join(task_dir, \"imagesTr\")\n",
    "labelsTr_dir = os.path.join(task_dir, \"labelsTr\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ 自動修正標籤檔：將 2 映射成 1\n",
    "# -------------------------------\n",
    "label_files = sorted(glob.glob(os.path.join(labelsTr_dir, \"*.nii.gz\")))\n",
    "for f in label_files:\n",
    "    img = nib.load(f)\n",
    "    data = img.get_fdata()\n",
    "    data[data == 2] = 1\n",
    "    nib.Nifti1Image(data.astype(np.uint8), img.affine, img.header).to_filename(f)\n",
    "print(\"✅ 標籤檔已修正完成 (2→1)\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ 生成 dataset.json\n",
    "# -------------------------------\n",
    "image_files = sorted(glob.glob(os.path.join(imagesTr_dir, \"*.nii.gz\")))\n",
    "\n",
    "dataset_json = {\n",
    "    \"name\": \"Dataset001\",\n",
    "    \"description\": \"Segmentation\",\n",
    "    \"tensorImageSize\": \"3D\",\n",
    "    \"modality\": {\"0\": \"CT\"},\n",
    "    \"labels\": {\"background\": 0, \"seg\": 1},  # ✅ 修正\n",
    "    \"numTraining\": len(image_files),\n",
    "    \"numTest\": 0,\n",
    "    \"file_ending\": \".nii.gz\",\n",
    "    \"channel_names\": {\"0\": \"CT\"},\n",
    "    \"training\": [\n",
    "        {\n",
    "            \"image\": os.path.join(\"imagesTr\", os.path.basename(img)),\n",
    "            \"label\": os.path.join(\"labelsTr\", os.path.basename(lbl))\n",
    "        }\n",
    "        for img, lbl in zip(image_files, label_files)\n",
    "    ],\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "\n",
    "dataset_json_path = os.path.join(task_dir, \"dataset.json\")\n",
    "with open(dataset_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_json, f, indent=4, ensure_ascii=False)\n",
    "print(\"✅ dataset.json 已生成，背景標籤正確\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ 設定 nnU-Net 環境變數\n",
    "# -------------------------------\n",
    "nnunet_raw = \"/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw\" #改成自己的帳號名稱放置路徑中\n",
    "nnunet_preprocessed = \"/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed\" #改成自己的帳號名稱放置路徑中\n",
    "nnunet_results = \"/home/sandy0317/practice_nnunet/nnUNet_results\" #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "os.environ[\"nnUNet_raw\"] = nnunet_raw\n",
    "os.environ[\"nnUNet_preprocessed\"] = nnunet_preprocessed\n",
    "os.environ[\"RESULTS_FOLDER\"] = nnunet_results\n",
    "os.environ[\"nnUNet_results\"] = nnunet_results  # ✅ 必須\n",
    "\n",
    "# 建立資料夾\n",
    "for p in [nnunet_raw, nnunet_preprocessed, nnunet_results]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    print(f\"✅ 資料夾確認: {p}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ 執行 nnU-Net 資料集預處理 (CLI)\n",
    "# -------------------------------\n",
    "\n",
    "cmd = [\n",
    "    \"/home/sandy0317/.local/bin/nnUNetv2_plan_and_preprocess\", #改成自己的帳號名稱放置路徑中\n",
    "    \"-p\", task_dir,            # 指定 Task 資料夾完整路徑\n",
    "    \"--verify_dataset_integrity\"\n",
    "]\n",
    "\n",
    "print(\"🚀 開始 nnU-Net 資料集預處理...\")\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "#!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity\n",
    "!/home/sandy0317/.local/bin/nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity\n",
    "print(\"✅ nnU-Net 資料集預處理完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5958ce6e-51d2-4c8c-acfa-97472e6b107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=2\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-10-17 08:51:37.559062: Using torch.compile...\n",
      "2025-10-17 08:51:40.368879: do_dummy_2d_data_aug: False\n",
      "2025-10-17 08:51:40.375142: Using splits from existing split file: /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed/Dataset001/splits_final.json\n",
      "2025-10-17 08:51:40.377495: The split file contains 5 splits.\n",
      "2025-10-17 08:51:40.377679: Desired fold for training: 0\n",
      "2025-10-17 08:51:40.377798: This split has 800 training and 200 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [127.0, 128.0, 128.0], 'spacing': [1.5, 1.5, 1.5], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 1.5, 1.5], 'original_median_shape_after_transp': [127, 128, 128], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.17309385538101196, 'median': 0.23840078711509705, 'min': -1.0, 'percentile_00_5': -0.9529684787988663, 'percentile_99_5': 0.8862527015805242, 'std': 0.36723610758781433}}} \n",
      "\n",
      "2025-10-17 08:51:42.834985: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-10-17 08:51:42.870868: \n",
      "2025-10-17 08:51:42.872210: Epoch 0\n",
      "2025-10-17 08:51:42.872785: Current learning rate: 0.01\n",
      "2025-10-17 08:54:59.105168: train_loss -0.5321\n",
      "2025-10-17 08:54:59.106425: val_loss -0.7252\n",
      "2025-10-17 08:54:59.106657: Pseudo dice [np.float32(0.9416)]\n",
      "2025-10-17 08:54:59.106893: Epoch time: 196.24 s\n",
      "2025-10-17 08:54:59.107100: Yayy! New best EMA pseudo Dice: 0.9416000247001648\n",
      "2025-10-17 08:55:03.309378: \n",
      "2025-10-17 08:55:03.309984: Epoch 1\n",
      "2025-10-17 08:55:03.310297: Current learning rate: 0.0091\n",
      "2025-10-17 08:57:09.994834: train_loss -0.7714\n",
      "2025-10-17 08:57:09.996237: val_loss -0.8599\n",
      "2025-10-17 08:57:09.996507: Pseudo dice [np.float32(0.9692)]\n",
      "2025-10-17 08:57:09.996754: Epoch time: 126.69 s\n",
      "2025-10-17 08:57:09.996936: Yayy! New best EMA pseudo Dice: 0.9444000124931335\n",
      "2025-10-17 08:57:15.466693: \n",
      "2025-10-17 08:57:15.467690: Epoch 2\n",
      "2025-10-17 08:57:15.467990: Current learning rate: 0.00818\n",
      "2025-10-17 08:59:22.090212: train_loss -0.8398\n",
      "2025-10-17 08:59:22.091532: val_loss -0.8857\n",
      "2025-10-17 08:59:22.091760: Pseudo dice [np.float32(0.9735)]\n",
      "2025-10-17 08:59:22.091992: Epoch time: 126.63 s\n",
      "2025-10-17 08:59:22.092177: Yayy! New best EMA pseudo Dice: 0.9473000168800354\n",
      "2025-10-17 08:59:27.019636: \n",
      "2025-10-17 08:59:27.020602: Epoch 3\n",
      "2025-10-17 08:59:27.020892: Current learning rate: 0.00725\n",
      "2025-10-17 09:01:28.268501: train_loss -0.8787\n",
      "2025-10-17 09:01:28.269987: val_loss -0.8959\n",
      "2025-10-17 09:01:28.270220: Pseudo dice [np.float32(0.9757)]\n",
      "2025-10-17 09:01:28.270497: Epoch time: 121.25 s\n",
      "2025-10-17 09:01:28.270676: Yayy! New best EMA pseudo Dice: 0.9502000212669373\n",
      "2025-10-17 09:01:33.051299: \n",
      "2025-10-17 09:01:33.052354: Epoch 4\n",
      "2025-10-17 09:01:33.052685: Current learning rate: 0.00631\n",
      "2025-10-17 09:03:39.910755: train_loss -0.8965\n",
      "2025-10-17 09:03:39.912065: val_loss -0.9014\n",
      "2025-10-17 09:03:39.912347: Pseudo dice [np.float32(0.9774)]\n",
      "2025-10-17 09:03:39.912602: Epoch time: 126.86 s\n",
      "2025-10-17 09:03:39.912779: Yayy! New best EMA pseudo Dice: 0.9528999924659729\n",
      "2025-10-17 09:03:44.617772: \n",
      "2025-10-17 09:03:44.618810: Epoch 5\n",
      "2025-10-17 09:03:44.619301: Current learning rate: 0.00536\n",
      "2025-10-17 09:05:51.311372: train_loss -0.9054\n",
      "2025-10-17 09:05:51.312505: val_loss -0.9176\n",
      "2025-10-17 09:05:51.312732: Pseudo dice [np.float32(0.9802)]\n",
      "2025-10-17 09:05:51.313003: Epoch time: 126.7 s\n",
      "2025-10-17 09:05:51.313183: Yayy! New best EMA pseudo Dice: 0.9556000232696533\n",
      "2025-10-17 09:05:55.859411: \n",
      "2025-10-17 09:05:55.862279: Epoch 6\n",
      "2025-10-17 09:05:55.862578: Current learning rate: 0.00438\n",
      "2025-10-17 09:08:01.797643: train_loss -0.909\n",
      "2025-10-17 09:08:01.799186: val_loss -0.9162\n",
      "2025-10-17 09:08:01.799445: Pseudo dice [np.float32(0.9801)]\n",
      "2025-10-17 09:08:01.799688: Epoch time: 125.94 s\n",
      "2025-10-17 09:08:01.799870: Yayy! New best EMA pseudo Dice: 0.9581000208854675\n",
      "2025-10-17 09:08:06.393687: \n",
      "2025-10-17 09:08:06.394728: Epoch 7\n",
      "2025-10-17 09:08:06.395029: Current learning rate: 0.00338\n",
      "2025-10-17 09:10:13.091942: train_loss -0.9133\n",
      "2025-10-17 09:10:13.093291: val_loss -0.9166\n",
      "2025-10-17 09:10:13.093521: Pseudo dice [np.float32(0.9802)]\n",
      "2025-10-17 09:10:13.093764: Epoch time: 126.7 s\n",
      "2025-10-17 09:10:13.093940: Yayy! New best EMA pseudo Dice: 0.9603000283241272\n",
      "2025-10-17 09:10:17.824292: \n",
      "2025-10-17 09:10:17.824974: Epoch 8\n",
      "2025-10-17 09:10:17.825333: Current learning rate: 0.00235\n",
      "2025-10-17 09:12:19.465390: train_loss -0.9163\n",
      "2025-10-17 09:12:19.466776: val_loss -0.9223\n",
      "2025-10-17 09:12:19.467020: Pseudo dice [np.float32(0.9811)]\n",
      "2025-10-17 09:12:19.467292: Epoch time: 121.64 s\n",
      "2025-10-17 09:12:19.467487: Yayy! New best EMA pseudo Dice: 0.9624000191688538\n",
      "2025-10-17 09:12:24.209478: \n",
      "2025-10-17 09:12:24.210342: Epoch 9\n",
      "2025-10-17 09:12:24.210754: Current learning rate: 0.00126\n",
      "2025-10-17 09:14:31.305105: train_loss -0.9189\n",
      "2025-10-17 09:14:31.308983: val_loss -0.9235\n",
      "2025-10-17 09:14:31.309230: Pseudo dice [np.float32(0.9814)]\n",
      "2025-10-17 09:14:31.309493: Epoch time: 127.1 s\n",
      "2025-10-17 09:14:31.309855: Yayy! New best EMA pseudo Dice: 0.9642999768257141\n",
      "2025-10-17 09:14:36.637533: Training done.\n",
      "2025-10-17 09:14:36.689929: Using splits from existing split file: /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed/Dataset001/splits_final.json\n",
      "2025-10-17 09:14:36.693650: The split file contains 5 splits.\n",
      "2025-10-17 09:14:36.694002: Desired fold for training: 0\n",
      "2025-10-17 09:14:36.694211: This split has 800 training and 200 validation cases.\n",
      "2025-10-17 09:14:36.708714: predicting seg_0000\n",
      "2025-10-17 09:14:36.770046: seg_0000, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:05.322738: predicting seg_0003\n",
      "2025-10-17 09:15:05.373832: seg_0003, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:06.080630: predicting seg_0004\n",
      "2025-10-17 09:15:06.119824: seg_0004, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:06.815638: predicting seg_0010\n",
      "2025-10-17 09:15:06.867745: seg_0010, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:07.561941: predicting seg_0017\n",
      "2025-10-17 09:15:07.600370: seg_0017, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:08.308623: predicting seg_0033\n",
      "2025-10-17 09:15:08.347786: seg_0033, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:09.064245: predicting seg_0041\n",
      "2025-10-17 09:15:09.101736: seg_0041, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:09.812340: predicting seg_0047\n",
      "2025-10-17 09:15:09.861709: seg_0047, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:10.562192: predicting seg_0053\n",
      "2025-10-17 09:15:10.604685: seg_0053, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:11.287931: predicting seg_0056\n",
      "2025-10-17 09:15:11.344568: seg_0056, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:12.023965: predicting seg_0060\n",
      "2025-10-17 09:15:12.069196: seg_0060, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:12.747501: predicting seg_0065\n",
      "2025-10-17 09:15:12.796824: seg_0065, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:13.469547: predicting seg_0073\n",
      "2025-10-17 09:15:13.510137: seg_0073, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:14.194887: predicting seg_0090\n",
      "2025-10-17 09:15:14.234219: seg_0090, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:14.907851: predicting seg_0101\n",
      "2025-10-17 09:15:14.946074: seg_0101, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:15.629787: predicting seg_0104\n",
      "2025-10-17 09:15:15.665719: seg_0104, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:16.336054: predicting seg_0106\n",
      "2025-10-17 09:15:16.382877: seg_0106, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:17.061738: predicting seg_0109\n",
      "2025-10-17 09:15:17.123740: seg_0109, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:17.780990: predicting seg_0111\n",
      "2025-10-17 09:15:17.820515: seg_0111, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:18.156820: predicting seg_0115\n",
      "2025-10-17 09:15:18.199146: seg_0115, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:18.541311: predicting seg_0117\n",
      "2025-10-17 09:15:18.571580: seg_0117, shape torch.Size([1, 89, 128, 121]), rank 0\n",
      "2025-10-17 09:15:18.908781: predicting seg_0127\n",
      "2025-10-17 09:15:18.949904: seg_0127, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:19.299134: predicting seg_0130\n",
      "2025-10-17 09:15:19.342471: seg_0130, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:19.710144: predicting seg_0133\n",
      "2025-10-17 09:15:19.757355: seg_0133, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:20.115899: predicting seg_0135\n",
      "2025-10-17 09:15:20.158779: seg_0135, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:20.498965: predicting seg_0136\n",
      "2025-10-17 09:15:20.539523: seg_0136, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:20.885585: predicting seg_0138\n",
      "2025-10-17 09:15:20.921929: seg_0138, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:21.256864: predicting seg_0144\n",
      "2025-10-17 09:15:21.314213: seg_0144, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:21.657225: predicting seg_0150\n",
      "2025-10-17 09:15:21.698130: seg_0150, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:22.043440: predicting seg_0153\n",
      "2025-10-17 09:15:22.079596: seg_0153, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:22.425674: predicting seg_0156\n",
      "2025-10-17 09:15:22.464174: seg_0156, shape torch.Size([1, 123, 128, 128]), rank 0\n",
      "2025-10-17 09:15:22.907185: predicting seg_0158\n",
      "2025-10-17 09:15:22.939842: seg_0158, shape torch.Size([1, 101, 128, 128]), rank 0\n",
      "2025-10-17 09:15:23.630368: predicting seg_0162\n",
      "2025-10-17 09:15:23.672411: seg_0162, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:24.378799: predicting seg_0164\n",
      "2025-10-17 09:15:24.423146: seg_0164, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:25.161872: predicting seg_0168\n",
      "2025-10-17 09:15:25.199614: seg_0168, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:25.910099: predicting seg_0173\n",
      "2025-10-17 09:15:25.949649: seg_0173, shape torch.Size([1, 118, 128, 121]), rank 0\n",
      "2025-10-17 09:15:26.663274: predicting seg_0181\n",
      "2025-10-17 09:15:26.700059: seg_0181, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:27.418550: predicting seg_0186\n",
      "2025-10-17 09:15:27.462121: seg_0186, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:28.187865: predicting seg_0187\n",
      "2025-10-17 09:15:28.237003: seg_0187, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:28.950552: predicting seg_0190\n",
      "2025-10-17 09:15:28.987763: seg_0190, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:29.687294: predicting seg_0196\n",
      "2025-10-17 09:15:29.732376: seg_0196, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:30.424837: predicting seg_0198\n",
      "2025-10-17 09:15:30.463959: seg_0198, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:31.163677: predicting seg_0199\n",
      "2025-10-17 09:15:31.206051: seg_0199, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:31.907133: predicting seg_0203\n",
      "2025-10-17 09:15:31.945119: seg_0203, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:32.643452: predicting seg_0207\n",
      "2025-10-17 09:15:32.683682: seg_0207, shape torch.Size([1, 123, 128, 128]), rank 0\n",
      "2025-10-17 09:15:33.385361: predicting seg_0216\n",
      "2025-10-17 09:15:33.424020: seg_0216, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-17 09:15:34.110930: predicting seg_0220\n",
      "2025-10-17 09:15:34.145053: seg_0220, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-17 09:15:34.861884: predicting seg_0222\n",
      "2025-10-17 09:15:34.892437: seg_0222, shape torch.Size([1, 95, 128, 125]), rank 0\n",
      "2025-10-17 09:15:35.591987: predicting seg_0238\n",
      "2025-10-17 09:15:35.625594: seg_0238, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:36.336495: predicting seg_0239\n",
      "2025-10-17 09:15:36.384224: seg_0239, shape torch.Size([1, 120, 128, 125]), rank 0\n",
      "2025-10-17 09:15:37.088072: predicting seg_0244\n",
      "2025-10-17 09:15:37.120211: seg_0244, shape torch.Size([1, 114, 128, 120]), rank 0\n",
      "2025-10-17 09:15:37.833910: predicting seg_0250\n",
      "2025-10-17 09:15:37.878678: seg_0250, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:38.595002: predicting seg_0254\n",
      "2025-10-17 09:15:38.631521: seg_0254, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:39.345183: predicting seg_0258\n",
      "2025-10-17 09:15:39.379203: seg_0258, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:40.085681: predicting seg_0262\n",
      "2025-10-17 09:15:40.129690: seg_0262, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:40.846578: predicting seg_0266\n",
      "2025-10-17 09:15:40.900996: seg_0266, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:41.627896: predicting seg_0268\n",
      "2025-10-17 09:15:41.671458: seg_0268, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:42.375794: predicting seg_0270\n",
      "2025-10-17 09:15:42.408663: seg_0270, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:43.110953: predicting seg_0280\n",
      "2025-10-17 09:15:43.145517: seg_0280, shape torch.Size([1, 110, 128, 117]), rank 0\n",
      "2025-10-17 09:15:43.852834: predicting seg_0281\n",
      "2025-10-17 09:15:43.893129: seg_0281, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:44.585685: predicting seg_0284\n",
      "2025-10-17 09:15:44.626118: seg_0284, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:45.317914: predicting seg_0286\n",
      "2025-10-17 09:15:45.348046: seg_0286, shape torch.Size([1, 78, 128, 128]), rank 0\n",
      "2025-10-17 09:15:46.047554: predicting seg_0296\n",
      "2025-10-17 09:15:46.088459: seg_0296, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-17 09:15:46.801930: predicting seg_0298\n",
      "2025-10-17 09:15:46.832785: seg_0298, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:47.524128: predicting seg_0304\n",
      "2025-10-17 09:15:47.564656: seg_0304, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:48.256370: predicting seg_0308\n",
      "2025-10-17 09:15:48.296642: seg_0308, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:48.990289: predicting seg_0311\n",
      "2025-10-17 09:15:49.028349: seg_0311, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:49.711533: predicting seg_0314\n",
      "2025-10-17 09:15:49.749037: seg_0314, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:50.470755: predicting seg_0320\n",
      "2025-10-17 09:15:50.510807: seg_0320, shape torch.Size([1, 127, 128, 124]), rank 0\n",
      "2025-10-17 09:15:51.203469: predicting seg_0322\n",
      "2025-10-17 09:15:51.240707: seg_0322, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-17 09:15:51.945206: predicting seg_0326\n",
      "2025-10-17 09:15:51.980330: seg_0326, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:52.678170: predicting seg_0327\n",
      "2025-10-17 09:15:52.714231: seg_0327, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:53.406233: predicting seg_0328\n",
      "2025-10-17 09:15:53.439786: seg_0328, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:54.131664: predicting seg_0332\n",
      "2025-10-17 09:15:54.171728: seg_0332, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:54.843286: predicting seg_0334\n",
      "2025-10-17 09:15:54.882198: seg_0334, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:55.574105: predicting seg_0343\n",
      "2025-10-17 09:15:55.607091: seg_0343, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-17 09:15:56.297803: predicting seg_0346\n",
      "2025-10-17 09:15:56.342477: seg_0346, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:15:57.033189: predicting seg_0352\n",
      "2025-10-17 09:15:57.070315: seg_0352, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:15:57.773764: predicting seg_0358\n",
      "2025-10-17 09:15:57.819193: seg_0358, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-17 09:15:58.525584: predicting seg_0370\n",
      "2025-10-17 09:15:58.580235: seg_0370, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-17 09:15:59.317909: predicting seg_0377\n",
      "2025-10-17 09:15:59.363596: seg_0377, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:00.085663: predicting seg_0380\n",
      "2025-10-17 09:16:00.132521: seg_0380, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:00.874831: predicting seg_0391\n",
      "2025-10-17 09:16:00.916155: seg_0391, shape torch.Size([1, 120, 128, 124]), rank 0\n",
      "2025-10-17 09:16:01.646123: predicting seg_0400\n",
      "2025-10-17 09:16:01.699683: seg_0400, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:02.402974: predicting seg_0404\n",
      "2025-10-17 09:16:02.473700: seg_0404, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:03.173704: predicting seg_0413\n",
      "2025-10-17 09:16:03.206093: seg_0413, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:03.934422: predicting seg_0422\n",
      "2025-10-17 09:16:03.981457: seg_0422, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:04.718591: predicting seg_0430\n",
      "2025-10-17 09:16:04.764135: seg_0430, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:05.483608: predicting seg_0435\n",
      "2025-10-17 09:16:05.538110: seg_0435, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:06.254612: predicting seg_0437\n",
      "2025-10-17 09:16:06.296337: seg_0437, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:06.999864: predicting seg_0440\n",
      "2025-10-17 09:16:07.041226: seg_0440, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:07.742821: predicting seg_0454\n",
      "2025-10-17 09:16:07.779697: seg_0454, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:08.481538: predicting seg_0458\n",
      "2025-10-17 09:16:08.515029: seg_0458, shape torch.Size([1, 87, 128, 128]), rank 0\n",
      "2025-10-17 09:16:09.215976: predicting seg_0462\n",
      "2025-10-17 09:16:09.256508: seg_0462, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:09.952006: predicting seg_0468\n",
      "2025-10-17 09:16:09.988520: seg_0468, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:10.684430: predicting seg_0469\n",
      "2025-10-17 09:16:10.735457: seg_0469, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:11.428411: predicting seg_0473\n",
      "2025-10-17 09:16:11.469609: seg_0473, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:12.158074: predicting seg_0474\n",
      "2025-10-17 09:16:12.195002: seg_0474, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:12.899007: predicting seg_0479\n",
      "2025-10-17 09:16:12.945246: seg_0479, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:13.645224: predicting seg_0481\n",
      "2025-10-17 09:16:13.681167: seg_0481, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:14.374458: predicting seg_0484\n",
      "2025-10-17 09:16:14.424977: seg_0484, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:15.157077: predicting seg_0488\n",
      "2025-10-17 09:16:15.214963: seg_0488, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:15.957278: predicting seg_0493\n",
      "2025-10-17 09:16:16.009311: seg_0493, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:16.731031: predicting seg_0503\n",
      "2025-10-17 09:16:16.777981: seg_0503, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-17 09:16:17.488821: predicting seg_0504\n",
      "2025-10-17 09:16:17.527574: seg_0504, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:18.243503: predicting seg_0509\n",
      "2025-10-17 09:16:18.284691: seg_0509, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:19.008790: predicting seg_0516\n",
      "2025-10-17 09:16:19.046300: seg_0516, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:19.743182: predicting seg_0517\n",
      "2025-10-17 09:16:19.788780: seg_0517, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:20.472293: predicting seg_0521\n",
      "2025-10-17 09:16:20.509863: seg_0521, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:21.210728: predicting seg_0540\n",
      "2025-10-17 09:16:21.248302: seg_0540, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:21.956833: predicting seg_0545\n",
      "2025-10-17 09:16:21.998187: seg_0545, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:22.707595: predicting seg_0551\n",
      "2025-10-17 09:16:22.740993: seg_0551, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:23.437591: predicting seg_0558\n",
      "2025-10-17 09:16:23.474245: seg_0558, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-17 09:16:24.172346: predicting seg_0560\n",
      "2025-10-17 09:16:24.208549: seg_0560, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:24.906532: predicting seg_0566\n",
      "2025-10-17 09:16:24.944371: seg_0566, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:25.662222: predicting seg_0569\n",
      "2025-10-17 09:16:25.710804: seg_0569, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:26.440536: predicting seg_0578\n",
      "2025-10-17 09:16:26.480251: seg_0578, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:27.200756: predicting seg_0582\n",
      "2025-10-17 09:16:27.236745: seg_0582, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:27.946132: predicting seg_0589\n",
      "2025-10-17 09:16:27.984770: seg_0589, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:28.712232: predicting seg_0590\n",
      "2025-10-17 09:16:28.754157: seg_0590, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:29.466025: predicting seg_0593\n",
      "2025-10-17 09:16:29.499881: seg_0593, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:30.212000: predicting seg_0599\n",
      "2025-10-17 09:16:30.253883: seg_0599, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:30.974552: predicting seg_0603\n",
      "2025-10-17 09:16:31.007877: seg_0603, shape torch.Size([1, 93, 128, 128]), rank 0\n",
      "2025-10-17 09:16:31.710760: predicting seg_0604\n",
      "2025-10-17 09:16:31.748837: seg_0604, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:32.442072: predicting seg_0606\n",
      "2025-10-17 09:16:32.475679: seg_0606, shape torch.Size([1, 109, 128, 128]), rank 0\n",
      "2025-10-17 09:16:33.177328: predicting seg_0609\n",
      "2025-10-17 09:16:33.210955: seg_0609, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:33.914217: predicting seg_0613\n",
      "2025-10-17 09:16:33.951026: seg_0613, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:34.642879: predicting seg_0617\n",
      "2025-10-17 09:16:34.686766: seg_0617, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:35.371021: predicting seg_0624\n",
      "2025-10-17 09:16:35.411734: seg_0624, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:36.142861: predicting seg_0625\n",
      "2025-10-17 09:16:36.188203: seg_0625, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:36.897440: predicting seg_0634\n",
      "2025-10-17 09:16:36.943198: seg_0634, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:37.673999: predicting seg_0656\n",
      "2025-10-17 09:16:37.727036: seg_0656, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:38.452335: predicting seg_0663\n",
      "2025-10-17 09:16:38.495453: seg_0663, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:39.217234: predicting seg_0664\n",
      "2025-10-17 09:16:39.257423: seg_0664, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:39.960051: predicting seg_0671\n",
      "2025-10-17 09:16:40.000330: seg_0671, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:40.688398: predicting seg_0677\n",
      "2025-10-17 09:16:40.726049: seg_0677, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-17 09:16:41.425091: predicting seg_0680\n",
      "2025-10-17 09:16:41.474387: seg_0680, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:42.187871: predicting seg_0696\n",
      "2025-10-17 09:16:42.229017: seg_0696, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:42.919453: predicting seg_0700\n",
      "2025-10-17 09:16:42.956302: seg_0700, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:43.675163: predicting seg_0703\n",
      "2025-10-17 09:16:43.715519: seg_0703, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:44.418515: predicting seg_0719\n",
      "2025-10-17 09:16:44.464872: seg_0719, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:45.165404: predicting seg_0725\n",
      "2025-10-17 09:16:45.205415: seg_0725, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:45.924953: predicting seg_0726\n",
      "2025-10-17 09:16:45.967568: seg_0726, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:46.684947: predicting seg_0727\n",
      "2025-10-17 09:16:46.723603: seg_0727, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:47.442827: predicting seg_0732\n",
      "2025-10-17 09:16:47.479676: seg_0732, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:48.195007: predicting seg_0733\n",
      "2025-10-17 09:16:48.232529: seg_0733, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:48.950477: predicting seg_0741\n",
      "2025-10-17 09:16:48.982924: seg_0741, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-17 09:16:49.697314: predicting seg_0743\n",
      "2025-10-17 09:16:49.736992: seg_0743, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-17 09:16:50.455888: predicting seg_0746\n",
      "2025-10-17 09:16:50.492482: seg_0746, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-17 09:16:51.211034: predicting seg_0754\n",
      "2025-10-17 09:16:51.248971: seg_0754, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:51.961470: predicting seg_0757\n",
      "2025-10-17 09:16:51.992672: seg_0757, shape torch.Size([1, 116, 128, 128]), rank 0\n",
      "2025-10-17 09:16:52.704378: predicting seg_0758\n",
      "2025-10-17 09:16:52.738773: seg_0758, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:53.447121: predicting seg_0761\n",
      "2025-10-17 09:16:53.479412: seg_0761, shape torch.Size([1, 95, 128, 128]), rank 0\n",
      "2025-10-17 09:16:54.153054: predicting seg_0765\n",
      "2025-10-17 09:16:54.189134: seg_0765, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:16:54.843734: predicting seg_0768\n",
      "2025-10-17 09:16:54.888441: seg_0768, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:55.570004: predicting seg_0771\n",
      "2025-10-17 09:16:55.609593: seg_0771, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:16:56.301857: predicting seg_0776\n",
      "2025-10-17 09:16:56.337187: seg_0776, shape torch.Size([1, 98, 128, 128]), rank 0\n",
      "2025-10-17 09:16:57.004805: predicting seg_0777\n",
      "2025-10-17 09:16:57.040473: seg_0777, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-17 09:16:57.707844: predicting seg_0787\n",
      "2025-10-17 09:16:57.737868: seg_0787, shape torch.Size([1, 99, 128, 128]), rank 0\n",
      "2025-10-17 09:16:58.420317: predicting seg_0795\n",
      "2025-10-17 09:16:58.458165: seg_0795, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-17 09:16:59.149046: predicting seg_0802\n",
      "2025-10-17 09:16:59.184237: seg_0802, shape torch.Size([1, 93, 128, 128]), rank 0\n",
      "2025-10-17 09:16:59.850102: predicting seg_0804\n",
      "2025-10-17 09:16:59.883459: seg_0804, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-17 09:17:00.556503: predicting seg_0821\n",
      "2025-10-17 09:17:00.596359: seg_0821, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:01.115527: predicting seg_0826\n",
      "2025-10-17 09:17:01.157676: seg_0826, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:01.494800: predicting seg_0829\n",
      "2025-10-17 09:17:01.528024: seg_0829, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:01.863682: predicting seg_0832\n",
      "2025-10-17 09:17:01.900937: seg_0832, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:02.233118: predicting seg_0833\n",
      "2025-10-17 09:17:02.269335: seg_0833, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:02.608013: predicting seg_0838\n",
      "2025-10-17 09:17:02.645194: seg_0838, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:02.981215: predicting seg_0853\n",
      "2025-10-17 09:17:03.011842: seg_0853, shape torch.Size([1, 87, 128, 118]), rank 0\n",
      "2025-10-17 09:17:03.344463: predicting seg_0854\n",
      "2025-10-17 09:17:03.383846: seg_0854, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-17 09:17:03.725643: predicting seg_0865\n",
      "2025-10-17 09:17:03.761037: seg_0865, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:04.098552: predicting seg_0878\n",
      "2025-10-17 09:17:04.143861: seg_0878, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:04.480233: predicting seg_0883\n",
      "2025-10-17 09:17:04.517019: seg_0883, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:04.852199: predicting seg_0884\n",
      "2025-10-17 09:17:04.886495: seg_0884, shape torch.Size([1, 103, 128, 128]), rank 0\n",
      "2025-10-17 09:17:05.223945: predicting seg_0889\n",
      "2025-10-17 09:17:05.269207: seg_0889, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:05.601780: predicting seg_0890\n",
      "2025-10-17 09:17:05.642953: seg_0890, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:06.242672: predicting seg_0893\n",
      "2025-10-17 09:17:06.275513: seg_0893, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:06.986502: predicting seg_0911\n",
      "2025-10-17 09:17:07.024937: seg_0911, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:07.706249: predicting seg_0912\n",
      "2025-10-17 09:17:07.753474: seg_0912, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:08.456976: predicting seg_0913\n",
      "2025-10-17 09:17:08.494929: seg_0913, shape torch.Size([1, 109, 128, 128]), rank 0\n",
      "2025-10-17 09:17:09.192792: predicting seg_0914\n",
      "2025-10-17 09:17:09.228545: seg_0914, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:09.908094: predicting seg_0915\n",
      "2025-10-17 09:17:09.944286: seg_0915, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:10.651142: predicting seg_0925\n",
      "2025-10-17 09:17:10.685549: seg_0925, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:11.373572: predicting seg_0937\n",
      "2025-10-17 09:17:11.408588: seg_0937, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:12.106642: predicting seg_0940\n",
      "2025-10-17 09:17:12.148441: seg_0940, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:12.844919: predicting seg_0942\n",
      "2025-10-17 09:17:12.886734: seg_0942, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-17 09:17:13.573147: predicting seg_0943\n",
      "2025-10-17 09:17:13.613431: seg_0943, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:14.304784: predicting seg_0954\n",
      "2025-10-17 09:17:14.332808: seg_0954, shape torch.Size([1, 107, 128, 128]), rank 0\n",
      "2025-10-17 09:17:15.018115: predicting seg_0955\n",
      "2025-10-17 09:17:15.056189: seg_0955, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:15.752951: predicting seg_0957\n",
      "2025-10-17 09:17:15.798603: seg_0957, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:16.500619: predicting seg_0961\n",
      "2025-10-17 09:17:16.533873: seg_0961, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-17 09:17:17.238293: predicting seg_0962\n",
      "2025-10-17 09:17:17.275509: seg_0962, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:17.989918: predicting seg_0966\n",
      "2025-10-17 09:17:18.038436: seg_0966, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:18.748723: predicting seg_0967\n",
      "2025-10-17 09:17:18.793663: seg_0967, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:19.525050: predicting seg_0974\n",
      "2025-10-17 09:17:19.571425: seg_0974, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:20.275189: predicting seg_0977\n",
      "2025-10-17 09:17:20.314247: seg_0977, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:20.987273: predicting seg_0979\n",
      "2025-10-17 09:17:21.028611: seg_0979, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:21.766245: predicting seg_0981\n",
      "2025-10-17 09:17:21.804988: seg_0981, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-17 09:17:22.518066: predicting seg_0982\n",
      "2025-10-17 09:17:22.550355: seg_0982, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:23.261401: predicting seg_0997\n",
      "2025-10-17 09:17:23.293016: seg_0997, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-17 09:17:42.306359: Validation complete\n",
      "2025-10-17 09:17:42.307227: Mean Validation Dice:  0.9816639526117998\n"
     ]
    }
   ],
   "source": [
    "#注意開始訓練前請記得至Terminal確認自己是否有安裝 GCC/G++ 編譯器(C 編譯器)\n",
    "#若無C編譯器，conda activate進入環境後執行\n",
    "#conda install -y -c conda-forge graphviz\n",
    "#conda install -y -c conda-forge gcc_linux-64 gxx_linux-64\n",
    "#指定環境變數\n",
    "#export CC=$CONDA_PREFIX/bin/x86_64-conda-linux-gnu-gcc\n",
    "#export CXX=$CONDA_PREFIX/bin/x86_64-conda-linux-gnu-g++\n",
    "\n",
    "import os\n",
    "\n",
    "# 先設定 CUDA_VISIBLE_DEVICES\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # 這裡改你想用的 GPU 編號\n",
    "\n",
    "# 再設定 nnU-Net 環境變數\n",
    "os.environ['CC'] = \"/home/sandy0317/.conda/envs/nnunet/bin/x86_64-conda-linux-gnu-gcc\"\n",
    "os.environ['CXX'] = \"/home/sandy0317/.conda/envs/nnunet/bin/x86_64-conda-linux-gnu-g++\"\n",
    "os.environ['nnUNet_raw'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw'\n",
    "os.environ['nnUNet_preprocessed'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed'\n",
    "os.environ['nnUNet_results'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_results'\n",
    "\n",
    "os.environ['C_INCLUDE_PATH'] = \"/home/sandy0317/.conda/envs/nnunet/include/python3.10\"\n",
    "os.environ['CPLUS_INCLUDE_PATH'] = \"/home/sandy0317/.conda/envs/nnunet/include/python3.10\"\n",
    "os.environ['LD_LIBRARY_PATH'] = \"/home/sandy0317/.conda/envs/nnunet/lib:\" + os.environ.get('LD_LIBRARY_PATH','')\n",
    "\n",
    "# 這行要放在環境變數都設定好「之後」\n",
    "!echo \"CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\"\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中，並更改指定GPU 編號\n",
    "#!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_train Dataset001 3d_fullres 0 -tr nnUNetTrainer_10epochs\n",
    "!CUDA_VISIBLE_DEVICES=2 /home/sandy0317/.local/bin/nnUNetv2_train Dataset001 3d_fullres 0 -tr nnUNetTrainer_10epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc999760-c288-4fe0-84b8-2bcd27ed8474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=6\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 3 cases in the source folder\n",
      "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 3 cases that I would like to predict\n",
      "\n",
      "Predicting sample_1:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.80s/it]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_1\n",
      "\n",
      "Predicting sample_2:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_2\n",
      "\n",
      "Predicting sample_3:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.79it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_3\n",
      "推論完成，結果存於： nnUNet_predictions\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 6. 推論 (Inference)\n",
    "# ===========================\n",
    "import os\n",
    "\n",
    "# 先設定 CUDA_VISIBLE_DEVICES\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"  # 這裡改你想用的 GPU 編號\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "os.environ['nnUNet_raw'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw'\n",
    "os.environ['nnUNet_preprocessed'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed'\n",
    "os.environ['nnUNet_results'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_results'\n",
    "\n",
    "# 從環境變數中讀取路徑\n",
    "nnUNet_raw = os.environ['nnUNet_raw']\n",
    "\n",
    "# 測試影像資料夾\n",
    "input_folder = os.path.join(nnUNet_raw, \"Dataset001\", \"imagesTs\")\n",
    "\n",
    "# 改成相對路徑或家目錄下\n",
    "output_folder = 'nnUNet_predictions'  # 或者 os.path.expanduser('~/nnUNet_predictions')\n",
    "\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 這行要放在環境變數都設定好「之後」\n",
    "!echo \"CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\"\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中，並更改指定GPU 編號\n",
    "#!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_predict -d 1 -i $input_folder -o $output_folder -tr nnUNetTrainer_10epochs -c 3d_fullres -f 0\n",
    "!CUDA_VISIBLE_DEVICES=6 /home/sandy0317/.local/bin/nnUNetv2_predict -d 1 -i $input_folder -o $output_folder -tr nnUNetTrainer_10epochs -c 3d_fullres -f 0\n",
    "\n",
    "print(\"推論完成，結果存於：\", output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c1ac2-ef6c-4272-8739-aa8864321c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 7. 驗證 (可選)\n",
    "# ===========================\n",
    "\n",
    "# 先設定 CUDA_VISIBLE_DEVICES\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"  # 這裡改你想用的 GPU 編號\n",
    "\n",
    "# 這行要放在環境變數都設定好「之後」\n",
    "!echo \"CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\"\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中，並更改指定GPU 編號\n",
    "#!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_evaluate_folder -ref $nnUNet_raw/Dataset001/labelsTr -pred $output_folder\n",
    "!CUDA_VISIBLE_DEVICES=6 /home/sandy0317/.local/bin/nnUNetv2_evaluate_folder -ref $nnUNet_raw/Dataset001/labelsTr -pred $output_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c3115-db8b-4387-a92c-e04bd855de43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa752444-d60f-4013-b606-7228d0c3b7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnU-Net)",
   "language": "python",
   "name": "nnunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

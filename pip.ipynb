{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "941bf0b2-bd0d-4ceb-a15f-e0315f2be862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnU-Net 資料夾已建立在 practice_nnunet/ 下\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 設定 nnU-Net 資料夾路徑\n",
    "base_dir = \"practice_nnunet\"\n",
    "nnUNet_raw = os.path.join(base_dir, \"nnUNet_raw\")\n",
    "nnUNet_preprocessed = os.path.join(base_dir, \"nnUNet_preprocessed\")\n",
    "nnUNet_results = os.path.join(base_dir, \"nnUNet_results\")\n",
    "\n",
    "# 建立資料夾\n",
    "os.makedirs(nnUNet_raw, exist_ok=True)\n",
    "os.makedirs(nnUNet_preprocessed, exist_ok=True)\n",
    "os.makedirs(nnUNet_results, exist_ok=True)\n",
    "\n",
    "# 設定環境變數\n",
    "os.environ[\"nnUNet_raw\"] = nnUNet_raw\n",
    "os.environ[\"nnUNet_preprocessed\"] = nnUNet_preprocessed\n",
    "os.environ[\"nnUNet_results\"] = nnUNet_results\n",
    "\n",
    "print(\"nnU-Net 資料夾已建立在 practice_nnunet/ 下\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d8e0cb-854c-409c-ad4b-5fe1e2956539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已完成轉換並複製 1000 筆資料到 nnU-Net 格式資料夾：/home/sandy0317/practice_nnunet/nnUNet_raw/Dataset001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# -------------------------------\n",
    "# nnU-Net 資料夾\n",
    "# -------------------------------\n",
    "task_dir = \"/home/sandy0317/practice_nnunet/nnUNet_raw/Dataset001\" #改成自己的帳號名稱放置路徑中\n",
    "imagesTr_dir = os.path.join(task_dir, \"imagesTr\")\n",
    "labelsTr_dir = os.path.join(task_dir, \"labelsTr\")\n",
    "os.makedirs(imagesTr_dir, exist_ok=True)\n",
    "os.makedirs(labelsTr_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 原始影像與 mask 資料夾\n",
    "# -------------------------------\n",
    "src_images = \"/home/sandy0317/Public/train/med-ddpm-1/image\" #改成自己的帳號名稱放置路徑中\n",
    "src_masks = \"/home/sandy0317/Public/train/med-ddpm-1/mask\" #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "# -------------------------------\n",
    "# 找出所有影像檔\n",
    "# -------------------------------\n",
    "image_paths = sorted(glob.glob(os.path.join(src_images, \"*\")))\n",
    "mask_paths = sorted(glob.glob(os.path.join(src_masks, \"*\")))\n",
    "\n",
    "# -------------------------------\n",
    "# 檢查數量一致\n",
    "# -------------------------------\n",
    "assert len(image_paths) == len(mask_paths), f\"影像數量({len(image_paths)})與標註數量({len(mask_paths)})不一致！\"\n",
    "\n",
    "# -------------------------------\n",
    "# 複製並重新命名\n",
    "# -------------------------------\n",
    "for idx, (img_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
    "    case_id = f\"seg_{idx:04d}\"\n",
    "\n",
    "    # nnU-Net 格式檔名\n",
    "    new_img_name = f\"{case_id}_0000.nii.gz\"  # 影像加 _0000\n",
    "    new_mask_name = f\"{case_id}.nii.gz\"      # mask 保留 case_id\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(imagesTr_dir, new_img_name))\n",
    "    shutil.copy(mask_path, os.path.join(labelsTr_dir, new_mask_name))\n",
    "\n",
    "print(f\"✅ 已完成轉換並複製 {len(image_paths)} 筆資料到 nnU-Net 格式資料夾：{task_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b9a2dd9-7f9c-400b-b22d-d0824594c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 標籤檔已修正完成 (2→1)\n",
      "✅ dataset.json 已生成，背景標籤正確\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnunet/nnUNet_raw\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnunet/nnUNet_preprocessed\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnunet/nnUNet_results\n",
      "🚀 開始 nnU-Net 資料集預處理...\n",
      "Fingerprint extraction...\n",
      "Dataset001\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:58<00:00, 17.05it/s]\n",
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Dropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [127. 128. 128.], 3d_lowres: [127, 128, 128]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 199, 'patch_size': (np.int64(128), np.int64(128)), 'median_image_size_in_voxels': array([128., 128.]), 'spacing': array([1.5, 1.5]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(128), np.int64(128), np.int64(128)), 'median_image_size_in_voxels': array([127., 128., 128.]), 'spacing': array([1.5, 1.5, 1.5]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n",
      "\n",
      "Plans were saved to /home/sandy0317/practice_nnunet/nnUNet_preprocessed/Dataset001/nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset001\n",
      "Configuration: 2d...\n",
      "100%|███████████████████████████████████████| 1000/1000 [14:25<00:00,  1.16it/s]\n",
      "Configuration: 3d_fullres...\n",
      "100%|███████████████████████████████████████| 1000/1000 [23:01<00:00,  1.38s/it]\n",
      "Configuration: 3d_lowres...\n",
      "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset001. Skipping.\n",
      "✅ nnU-Net 資料集預處理完成\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# nnU-Net Dataset 前置流程 (Jupyter Notebook)\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ 原始資料集路徑\n",
    "# -------------------------------\n",
    "task_dir = \"/home/sandy0317/practice_nnunet/nnUNet_raw/Dataset001\" #改成自己的帳號名稱放置路徑中\n",
    "imagesTr_dir = os.path.join(task_dir, \"imagesTr\")\n",
    "labelsTr_dir = os.path.join(task_dir, \"labelsTr\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ 自動修正標籤檔：將 2 映射成 1\n",
    "# -------------------------------\n",
    "label_files = sorted(glob.glob(os.path.join(labelsTr_dir, \"*.nii.gz\")))\n",
    "for f in label_files:\n",
    "    img = nib.load(f)\n",
    "    data = img.get_fdata()\n",
    "    data[data == 2] = 1\n",
    "    nib.Nifti1Image(data.astype(np.uint8), img.affine, img.header).to_filename(f)\n",
    "print(\"✅ 標籤檔已修正完成 (2→1)\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ 生成 dataset.json\n",
    "# -------------------------------\n",
    "image_files = sorted(glob.glob(os.path.join(imagesTr_dir, \"*.nii.gz\")))\n",
    "\n",
    "dataset_json = {\n",
    "    \"name\": \"Dataset001\",\n",
    "    \"description\": \"Segmentation\",\n",
    "    \"tensorImageSize\": \"3D\",\n",
    "    \"modality\": {\"0\": \"CT\"},\n",
    "    \"labels\": {\"background\": 0, \"seg\": 1},  # ✅ 修正\n",
    "    \"numTraining\": len(image_files),\n",
    "    \"numTest\": 0,\n",
    "    \"file_ending\": \".nii.gz\",\n",
    "    \"channel_names\": {\"0\": \"CT\"},\n",
    "    \"training\": [\n",
    "        {\n",
    "            \"image\": os.path.join(\"imagesTr\", os.path.basename(img)),\n",
    "            \"label\": os.path.join(\"labelsTr\", os.path.basename(lbl))\n",
    "        }\n",
    "        for img, lbl in zip(image_files, label_files)\n",
    "    ],\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "\n",
    "dataset_json_path = os.path.join(task_dir, \"dataset.json\")\n",
    "with open(dataset_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_json, f, indent=4, ensure_ascii=False)\n",
    "print(\"✅ dataset.json 已生成，背景標籤正確\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ 設定 nnU-Net 環境變數\n",
    "# -------------------------------\n",
    "nnunet_raw = \"/home/sandy0317/practice_nnunet/nnUNet_raw\" #改成自己的帳號名稱放置路徑中\n",
    "nnunet_preprocessed = \"/home/sandy0317/practice_nnunet/nnUNet_preprocessed\" #改成自己的帳號名稱放置路徑中\n",
    "nnunet_results = \"/home/sandy0317/practice_nnunet/nnUNet_results\" #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "os.environ[\"nnUNet_raw\"] = nnunet_raw\n",
    "os.environ[\"nnUNet_preprocessed\"] = nnunet_preprocessed\n",
    "os.environ[\"RESULTS_FOLDER\"] = nnunet_results\n",
    "os.environ[\"nnUNet_results\"] = nnunet_results  # ✅ 必須\n",
    "\n",
    "# 建立資料夾\n",
    "for p in [nnunet_raw, nnunet_preprocessed, nnunet_results]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    print(f\"✅ 資料夾確認: {p}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ 執行 nnU-Net 資料集預處理 (CLI)\n",
    "# -------------------------------\n",
    "cmd = [\n",
    "    \"/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_plan_and_preprocess\", #改成自己的帳號名稱放置路徑中\n",
    "    \"-p\", task_dir,            # 指定 Task 資料夾完整路徑\n",
    "    \"--verify_dataset_integrity\"\n",
    "]\n",
    "\n",
    "print(\"🚀 開始 nnU-Net 資料集預處理...\")\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity\n",
    "print(\"✅ nnU-Net 資料集預處理完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5958ce6e-51d2-4c8c-acfa-97472e6b107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-10-02 07:23:05.573032: Using torch.compile...\n",
      "2025-10-02 07:23:07.868888: do_dummy_2d_data_aug: False\n",
      "2025-10-02 07:23:07.874792: Creating new 5-fold cross-validation split...\n",
      "2025-10-02 07:23:07.892858: Desired fold for training: 0\n",
      "2025-10-02 07:23:07.893035: This split has 800 training and 200 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [127.0, 128.0, 128.0], 'spacing': [1.5, 1.5, 1.5], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 1.5, 1.5], 'original_median_shape_after_transp': [127, 128, 128], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.17309385538101196, 'median': 0.23840078711509705, 'min': -1.0, 'percentile_00_5': -0.9529684787988663, 'percentile_99_5': 0.8862527015805242, 'std': 0.36723610758781433}}} \n",
      "\n",
      "2025-10-02 07:23:10.168053: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-10-02 07:23:10.191548: \n",
      "2025-10-02 07:23:10.191977: Epoch 0\n",
      "2025-10-02 07:23:10.192547: Current learning rate: 0.01\n",
      "2025-10-02 07:25:05.556777: train_loss -0.6131\n",
      "2025-10-02 07:25:05.557879: val_loss -0.8495\n",
      "2025-10-02 07:25:05.558095: Pseudo dice [np.float32(0.9657)]\n",
      "2025-10-02 07:25:05.558330: Epoch time: 115.37 s\n",
      "2025-10-02 07:25:05.558501: Yayy! New best EMA pseudo Dice: 0.9656999707221985\n",
      "2025-10-02 07:25:09.145554: \n",
      "2025-10-02 07:25:09.146075: Epoch 1\n",
      "2025-10-02 07:25:09.146454: Current learning rate: 0.0091\n",
      "2025-10-02 07:26:11.673154: train_loss -0.8039\n",
      "2025-10-02 07:26:11.674191: val_loss -0.8623\n",
      "2025-10-02 07:26:11.674437: Pseudo dice [np.float32(0.9678)]\n",
      "2025-10-02 07:26:11.674661: Epoch time: 62.53 s\n",
      "2025-10-02 07:26:11.674830: Yayy! New best EMA pseudo Dice: 0.9659000039100647\n",
      "2025-10-02 07:26:15.871649: \n",
      "2025-10-02 07:26:15.872097: Epoch 2\n",
      "2025-10-02 07:26:15.872531: Current learning rate: 0.00818\n",
      "2025-10-02 07:27:18.353583: train_loss -0.8614\n",
      "2025-10-02 07:27:18.354552: val_loss -0.8901\n",
      "2025-10-02 07:27:18.354778: Pseudo dice [np.float32(0.9742)]\n",
      "2025-10-02 07:27:18.354994: Epoch time: 62.49 s\n",
      "2025-10-02 07:27:18.355166: Yayy! New best EMA pseudo Dice: 0.96670001745224\n",
      "2025-10-02 07:27:23.477321: \n",
      "2025-10-02 07:27:23.477860: Epoch 3\n",
      "2025-10-02 07:27:23.478308: Current learning rate: 0.00725\n",
      "2025-10-02 07:28:25.969690: train_loss -0.8893\n",
      "2025-10-02 07:28:25.970727: val_loss -0.9092\n",
      "2025-10-02 07:28:25.970996: Pseudo dice [np.float32(0.978)]\n",
      "2025-10-02 07:28:25.971239: Epoch time: 62.5 s\n",
      "2025-10-02 07:28:25.971440: Yayy! New best EMA pseudo Dice: 0.9678000211715698\n",
      "2025-10-02 07:28:30.252988: \n",
      "2025-10-02 07:28:30.253673: Epoch 4\n",
      "2025-10-02 07:28:30.254000: Current learning rate: 0.00631\n",
      "2025-10-02 07:29:32.762375: train_loss -0.9069\n",
      "2025-10-02 07:29:32.763073: val_loss -0.9162\n",
      "2025-10-02 07:29:32.763307: Pseudo dice [np.float32(0.98)]\n",
      "2025-10-02 07:29:32.763527: Epoch time: 62.51 s\n",
      "2025-10-02 07:29:32.763698: Yayy! New best EMA pseudo Dice: 0.9690999984741211\n",
      "2025-10-02 07:29:37.165785: \n",
      "2025-10-02 07:29:37.166521: Epoch 5\n",
      "2025-10-02 07:29:37.166827: Current learning rate: 0.00536\n",
      "2025-10-02 07:30:39.590591: train_loss -0.9134\n",
      "2025-10-02 07:30:39.591416: val_loss -0.9202\n",
      "2025-10-02 07:30:39.591632: Pseudo dice [np.float32(0.9806)]\n",
      "2025-10-02 07:30:39.591852: Epoch time: 62.43 s\n",
      "2025-10-02 07:30:39.592027: Yayy! New best EMA pseudo Dice: 0.9702000021934509\n",
      "2025-10-02 07:30:43.832674: \n",
      "2025-10-02 07:30:43.833366: Epoch 6\n",
      "2025-10-02 07:30:43.833638: Current learning rate: 0.00438\n",
      "2025-10-02 07:31:46.287710: train_loss -0.9159\n",
      "2025-10-02 07:31:46.288466: val_loss -0.9212\n",
      "2025-10-02 07:31:46.288730: Pseudo dice [np.float32(0.9809)]\n",
      "2025-10-02 07:31:46.288957: Epoch time: 62.46 s\n",
      "2025-10-02 07:31:46.289128: Yayy! New best EMA pseudo Dice: 0.9713000059127808\n",
      "2025-10-02 07:31:50.602158: \n",
      "2025-10-02 07:31:50.602751: Epoch 7\n",
      "2025-10-02 07:31:50.603079: Current learning rate: 0.00338\n",
      "2025-10-02 07:32:53.048223: train_loss -0.9209\n",
      "2025-10-02 07:32:53.048945: val_loss -0.9226\n",
      "2025-10-02 07:32:53.049199: Pseudo dice [np.float32(0.9805)]\n",
      "2025-10-02 07:32:53.049435: Epoch time: 62.45 s\n",
      "2025-10-02 07:32:53.049605: Yayy! New best EMA pseudo Dice: 0.9721999764442444\n",
      "2025-10-02 07:32:57.409756: \n",
      "2025-10-02 07:32:57.410344: Epoch 8\n",
      "2025-10-02 07:32:57.410735: Current learning rate: 0.00235\n",
      "2025-10-02 07:34:00.180460: train_loss -0.9209\n",
      "2025-10-02 07:34:00.181199: val_loss -0.9274\n",
      "2025-10-02 07:34:00.181557: Pseudo dice [np.float32(0.9822)]\n",
      "2025-10-02 07:34:00.181829: Epoch time: 62.77 s\n",
      "2025-10-02 07:34:00.182010: Yayy! New best EMA pseudo Dice: 0.9732000231742859\n",
      "2025-10-02 07:34:04.588729: \n",
      "2025-10-02 07:34:04.589325: Epoch 9\n",
      "2025-10-02 07:34:04.589655: Current learning rate: 0.00126\n",
      "2025-10-02 07:35:07.333590: train_loss -0.9223\n",
      "2025-10-02 07:35:07.334426: val_loss -0.9304\n",
      "2025-10-02 07:35:07.334691: Pseudo dice [np.float32(0.9827)]\n",
      "2025-10-02 07:35:07.334931: Epoch time: 62.75 s\n",
      "2025-10-02 07:35:07.335133: Yayy! New best EMA pseudo Dice: 0.9740999937057495\n",
      "2025-10-02 07:35:12.166492: Training done.\n",
      "2025-10-02 07:35:12.208030: Using splits from existing split file: practice_nnunet/nnUNet_preprocessed/Dataset001/splits_final.json\n",
      "2025-10-02 07:35:12.211696: The split file contains 5 splits.\n",
      "2025-10-02 07:35:12.211966: Desired fold for training: 0\n",
      "2025-10-02 07:35:12.212142: This split has 800 training and 200 validation cases.\n",
      "2025-10-02 07:35:12.227087: predicting seg_0000\n",
      "2025-10-02 07:35:12.286154: seg_0000, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:35.653579: predicting seg_0003\n",
      "2025-10-02 07:35:35.699123: seg_0003, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:36.021047: predicting seg_0004\n",
      "2025-10-02 07:35:36.060569: seg_0004, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:36.373423: predicting seg_0010\n",
      "2025-10-02 07:35:36.416440: seg_0010, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:36.728132: predicting seg_0017\n",
      "2025-10-02 07:35:36.764197: seg_0017, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:37.076017: predicting seg_0033\n",
      "2025-10-02 07:35:37.110203: seg_0033, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:37.422639: predicting seg_0041\n",
      "2025-10-02 07:35:37.460232: seg_0041, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:37.769343: predicting seg_0047\n",
      "2025-10-02 07:35:37.816855: seg_0047, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:38.133162: predicting seg_0053\n",
      "2025-10-02 07:35:38.173965: seg_0053, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:38.483938: predicting seg_0056\n",
      "2025-10-02 07:35:38.526433: seg_0056, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:38.838697: predicting seg_0060\n",
      "2025-10-02 07:35:38.882057: seg_0060, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:39.195065: predicting seg_0065\n",
      "2025-10-02 07:35:39.239961: seg_0065, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:39.552062: predicting seg_0073\n",
      "2025-10-02 07:35:39.589437: seg_0073, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:39.901444: predicting seg_0090\n",
      "2025-10-02 07:35:39.939337: seg_0090, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:40.252089: predicting seg_0101\n",
      "2025-10-02 07:35:40.287091: seg_0101, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:40.598467: predicting seg_0104\n",
      "2025-10-02 07:35:40.632209: seg_0104, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:40.942909: predicting seg_0106\n",
      "2025-10-02 07:35:40.984918: seg_0106, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:41.300145: predicting seg_0109\n",
      "2025-10-02 07:35:41.338677: seg_0109, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:41.658085: predicting seg_0111\n",
      "2025-10-02 07:35:41.698364: seg_0111, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:42.009990: predicting seg_0115\n",
      "2025-10-02 07:35:42.044363: seg_0115, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:42.354462: predicting seg_0117\n",
      "2025-10-02 07:35:42.384268: seg_0117, shape torch.Size([1, 89, 128, 121]), rank 0\n",
      "2025-10-02 07:35:42.697764: predicting seg_0127\n",
      "2025-10-02 07:35:42.735879: seg_0127, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:43.055893: predicting seg_0130\n",
      "2025-10-02 07:35:43.092863: seg_0130, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:43.410180: predicting seg_0133\n",
      "2025-10-02 07:35:43.446030: seg_0133, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:43.760148: predicting seg_0135\n",
      "2025-10-02 07:35:43.798659: seg_0135, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:44.123927: predicting seg_0136\n",
      "2025-10-02 07:35:44.163035: seg_0136, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:44.479834: predicting seg_0138\n",
      "2025-10-02 07:35:44.514991: seg_0138, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:44.827373: predicting seg_0144\n",
      "2025-10-02 07:35:44.861271: seg_0144, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:45.172278: predicting seg_0150\n",
      "2025-10-02 07:35:45.207944: seg_0150, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:45.529025: predicting seg_0153\n",
      "2025-10-02 07:35:45.565717: seg_0153, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:45.881436: predicting seg_0156\n",
      "2025-10-02 07:35:45.917212: seg_0156, shape torch.Size([1, 123, 128, 128]), rank 0\n",
      "2025-10-02 07:35:46.234104: predicting seg_0158\n",
      "2025-10-02 07:35:46.265346: seg_0158, shape torch.Size([1, 101, 128, 128]), rank 0\n",
      "2025-10-02 07:35:46.576823: predicting seg_0162\n",
      "2025-10-02 07:35:46.612537: seg_0162, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:46.924635: predicting seg_0164\n",
      "2025-10-02 07:35:46.960308: seg_0164, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:47.271606: predicting seg_0168\n",
      "2025-10-02 07:35:47.307168: seg_0168, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:47.623658: predicting seg_0173\n",
      "2025-10-02 07:35:47.660804: seg_0173, shape torch.Size([1, 118, 128, 121]), rank 0\n",
      "2025-10-02 07:35:47.974035: predicting seg_0181\n",
      "2025-10-02 07:35:48.009358: seg_0181, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:48.328244: predicting seg_0186\n",
      "2025-10-02 07:35:48.374818: seg_0186, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:48.689908: predicting seg_0187\n",
      "2025-10-02 07:35:48.733880: seg_0187, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:49.043308: predicting seg_0190\n",
      "2025-10-02 07:35:49.078647: seg_0190, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:49.390054: predicting seg_0196\n",
      "2025-10-02 07:35:49.431530: seg_0196, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:49.741086: predicting seg_0198\n",
      "2025-10-02 07:35:49.776373: seg_0198, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:50.090889: predicting seg_0199\n",
      "2025-10-02 07:35:50.124087: seg_0199, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:50.433390: predicting seg_0203\n",
      "2025-10-02 07:35:50.468917: seg_0203, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:50.782268: predicting seg_0207\n",
      "2025-10-02 07:35:50.820807: seg_0207, shape torch.Size([1, 123, 128, 128]), rank 0\n",
      "2025-10-02 07:35:51.135029: predicting seg_0216\n",
      "2025-10-02 07:35:51.171948: seg_0216, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-02 07:35:51.487441: predicting seg_0220\n",
      "2025-10-02 07:35:51.524647: seg_0220, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-02 07:35:51.839842: predicting seg_0222\n",
      "2025-10-02 07:35:51.867184: seg_0222, shape torch.Size([1, 95, 128, 125]), rank 0\n",
      "2025-10-02 07:35:52.178370: predicting seg_0238\n",
      "2025-10-02 07:35:52.214339: seg_0238, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:52.527284: predicting seg_0239\n",
      "2025-10-02 07:35:52.563701: seg_0239, shape torch.Size([1, 120, 128, 125]), rank 0\n",
      "2025-10-02 07:35:52.881149: predicting seg_0244\n",
      "2025-10-02 07:35:52.914175: seg_0244, shape torch.Size([1, 114, 128, 120]), rank 0\n",
      "2025-10-02 07:35:53.227651: predicting seg_0250\n",
      "2025-10-02 07:35:53.266216: seg_0250, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:53.587713: predicting seg_0254\n",
      "2025-10-02 07:35:53.622340: seg_0254, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:53.934801: predicting seg_0258\n",
      "2025-10-02 07:35:53.971044: seg_0258, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:54.282762: predicting seg_0262\n",
      "2025-10-02 07:35:54.320997: seg_0262, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:54.635689: predicting seg_0266\n",
      "2025-10-02 07:35:54.674436: seg_0266, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:54.988210: predicting seg_0268\n",
      "2025-10-02 07:35:55.028487: seg_0268, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:55.341880: predicting seg_0270\n",
      "2025-10-02 07:35:55.375521: seg_0270, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:55.691693: predicting seg_0280\n",
      "2025-10-02 07:35:55.725736: seg_0280, shape torch.Size([1, 110, 128, 117]), rank 0\n",
      "2025-10-02 07:35:56.039056: predicting seg_0281\n",
      "2025-10-02 07:35:56.076710: seg_0281, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:56.387043: predicting seg_0284\n",
      "2025-10-02 07:35:56.424588: seg_0284, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:56.737980: predicting seg_0286\n",
      "2025-10-02 07:35:56.766820: seg_0286, shape torch.Size([1, 78, 128, 128]), rank 0\n",
      "2025-10-02 07:35:57.077044: predicting seg_0296\n",
      "2025-10-02 07:35:57.117287: seg_0296, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-02 07:35:57.430373: predicting seg_0298\n",
      "2025-10-02 07:35:57.461280: seg_0298, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:57.772456: predicting seg_0304\n",
      "2025-10-02 07:35:57.811361: seg_0304, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:35:58.123591: predicting seg_0308\n",
      "2025-10-02 07:35:58.163337: seg_0308, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:58.478223: predicting seg_0311\n",
      "2025-10-02 07:35:58.518637: seg_0311, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:58.834702: predicting seg_0314\n",
      "2025-10-02 07:35:58.869848: seg_0314, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:35:59.182251: predicting seg_0320\n",
      "2025-10-02 07:35:59.220990: seg_0320, shape torch.Size([1, 127, 128, 124]), rank 0\n",
      "2025-10-02 07:35:59.548740: predicting seg_0322\n",
      "2025-10-02 07:35:59.584560: seg_0322, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-02 07:35:59.897531: predicting seg_0326\n",
      "2025-10-02 07:35:59.932250: seg_0326, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:00.244878: predicting seg_0327\n",
      "2025-10-02 07:36:00.280524: seg_0327, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:00.593528: predicting seg_0328\n",
      "2025-10-02 07:36:00.629194: seg_0328, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:00.938124: predicting seg_0332\n",
      "2025-10-02 07:36:00.978351: seg_0332, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:01.291667: predicting seg_0334\n",
      "2025-10-02 07:36:01.330615: seg_0334, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:01.646014: predicting seg_0343\n",
      "2025-10-02 07:36:01.678814: seg_0343, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-02 07:36:01.990400: predicting seg_0346\n",
      "2025-10-02 07:36:02.032187: seg_0346, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:02.344289: predicting seg_0352\n",
      "2025-10-02 07:36:02.382594: seg_0352, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:02.700796: predicting seg_0358\n",
      "2025-10-02 07:36:02.741970: seg_0358, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-02 07:36:03.056534: predicting seg_0370\n",
      "2025-10-02 07:36:03.095062: seg_0370, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-02 07:36:03.411529: predicting seg_0377\n",
      "2025-10-02 07:36:03.449225: seg_0377, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:03.764328: predicting seg_0380\n",
      "2025-10-02 07:36:03.801010: seg_0380, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:04.125610: predicting seg_0391\n",
      "2025-10-02 07:36:04.162590: seg_0391, shape torch.Size([1, 120, 128, 124]), rank 0\n",
      "2025-10-02 07:36:04.480818: predicting seg_0400\n",
      "2025-10-02 07:36:04.518452: seg_0400, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:04.831621: predicting seg_0404\n",
      "2025-10-02 07:36:04.874496: seg_0404, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:05.187531: predicting seg_0413\n",
      "2025-10-02 07:36:05.219889: seg_0413, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:05.535407: predicting seg_0422\n",
      "2025-10-02 07:36:05.570128: seg_0422, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:05.886084: predicting seg_0430\n",
      "2025-10-02 07:36:05.923442: seg_0430, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:06.241289: predicting seg_0435\n",
      "2025-10-02 07:36:06.278035: seg_0435, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:06.597404: predicting seg_0437\n",
      "2025-10-02 07:36:06.633718: seg_0437, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:06.948028: predicting seg_0440\n",
      "2025-10-02 07:36:06.985890: seg_0440, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:07.301141: predicting seg_0454\n",
      "2025-10-02 07:36:07.334168: seg_0454, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:07.646369: predicting seg_0458\n",
      "2025-10-02 07:36:07.677364: seg_0458, shape torch.Size([1, 87, 128, 128]), rank 0\n",
      "2025-10-02 07:36:07.988517: predicting seg_0462\n",
      "2025-10-02 07:36:08.024168: seg_0462, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:08.336563: predicting seg_0468\n",
      "2025-10-02 07:36:08.372807: seg_0468, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:08.687393: predicting seg_0469\n",
      "2025-10-02 07:36:08.721790: seg_0469, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:09.036185: predicting seg_0473\n",
      "2025-10-02 07:36:09.072487: seg_0473, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:09.388028: predicting seg_0474\n",
      "2025-10-02 07:36:09.423021: seg_0474, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:09.738177: predicting seg_0479\n",
      "2025-10-02 07:36:09.774204: seg_0479, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:10.087318: predicting seg_0481\n",
      "2025-10-02 07:36:10.120069: seg_0481, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:10.435495: predicting seg_0484\n",
      "2025-10-02 07:36:10.468709: seg_0484, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:10.781323: predicting seg_0488\n",
      "2025-10-02 07:36:10.817868: seg_0488, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:11.131185: predicting seg_0493\n",
      "2025-10-02 07:36:11.173044: seg_0493, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:11.493351: predicting seg_0503\n",
      "2025-10-02 07:36:11.539353: seg_0503, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-02 07:36:11.855423: predicting seg_0504\n",
      "2025-10-02 07:36:11.891725: seg_0504, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:12.214063: predicting seg_0509\n",
      "2025-10-02 07:36:12.252473: seg_0509, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:12.567282: predicting seg_0516\n",
      "2025-10-02 07:36:12.602523: seg_0516, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:12.915723: predicting seg_0517\n",
      "2025-10-02 07:36:12.957371: seg_0517, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:13.271723: predicting seg_0521\n",
      "2025-10-02 07:36:13.304215: seg_0521, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:13.617516: predicting seg_0540\n",
      "2025-10-02 07:36:13.651881: seg_0540, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:13.964124: predicting seg_0545\n",
      "2025-10-02 07:36:14.000924: seg_0545, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:14.312790: predicting seg_0551\n",
      "2025-10-02 07:36:14.346924: seg_0551, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:14.660118: predicting seg_0558\n",
      "2025-10-02 07:36:14.694728: seg_0558, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-02 07:36:15.010160: predicting seg_0560\n",
      "2025-10-02 07:36:15.045651: seg_0560, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:15.356021: predicting seg_0566\n",
      "2025-10-02 07:36:15.392622: seg_0566, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:15.705324: predicting seg_0569\n",
      "2025-10-02 07:36:15.747567: seg_0569, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:16.061454: predicting seg_0578\n",
      "2025-10-02 07:36:16.100437: seg_0578, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:16.414954: predicting seg_0582\n",
      "2025-10-02 07:36:16.448614: seg_0582, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:16.764302: predicting seg_0589\n",
      "2025-10-02 07:36:16.801024: seg_0589, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:17.113344: predicting seg_0590\n",
      "2025-10-02 07:36:17.152745: seg_0590, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:17.462978: predicting seg_0593\n",
      "2025-10-02 07:36:17.497453: seg_0593, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:17.810729: predicting seg_0599\n",
      "2025-10-02 07:36:17.851443: seg_0599, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:18.168761: predicting seg_0603\n",
      "2025-10-02 07:36:18.199168: seg_0603, shape torch.Size([1, 93, 128, 128]), rank 0\n",
      "2025-10-02 07:36:18.510895: predicting seg_0604\n",
      "2025-10-02 07:36:18.547419: seg_0604, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:18.859067: predicting seg_0606\n",
      "2025-10-02 07:36:18.892144: seg_0606, shape torch.Size([1, 109, 128, 128]), rank 0\n",
      "2025-10-02 07:36:19.207867: predicting seg_0609\n",
      "2025-10-02 07:36:19.242272: seg_0609, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:19.557025: predicting seg_0613\n",
      "2025-10-02 07:36:19.591652: seg_0613, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:19.907594: predicting seg_0617\n",
      "2025-10-02 07:36:19.943606: seg_0617, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:20.254840: predicting seg_0624\n",
      "2025-10-02 07:36:20.288556: seg_0624, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:20.603326: predicting seg_0625\n",
      "2025-10-02 07:36:20.639678: seg_0625, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:20.954751: predicting seg_0634\n",
      "2025-10-02 07:36:20.997594: seg_0634, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:21.309119: predicting seg_0656\n",
      "2025-10-02 07:36:21.354465: seg_0656, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:21.667006: predicting seg_0663\n",
      "2025-10-02 07:36:21.705134: seg_0663, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:22.019227: predicting seg_0664\n",
      "2025-10-02 07:36:22.057080: seg_0664, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:22.373711: predicting seg_0671\n",
      "2025-10-02 07:36:22.413129: seg_0671, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:22.730112: predicting seg_0677\n",
      "2025-10-02 07:36:22.763140: seg_0677, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-02 07:36:23.078388: predicting seg_0680\n",
      "2025-10-02 07:36:23.115724: seg_0680, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:23.432181: predicting seg_0696\n",
      "2025-10-02 07:36:23.470026: seg_0696, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:23.786249: predicting seg_0700\n",
      "2025-10-02 07:36:23.835091: seg_0700, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:24.147266: predicting seg_0703\n",
      "2025-10-02 07:36:24.183705: seg_0703, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:24.499465: predicting seg_0719\n",
      "2025-10-02 07:36:24.538308: seg_0719, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:24.853767: predicting seg_0725\n",
      "2025-10-02 07:36:24.891733: seg_0725, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:25.206044: predicting seg_0726\n",
      "2025-10-02 07:36:25.244705: seg_0726, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:25.559397: predicting seg_0727\n",
      "2025-10-02 07:36:25.599070: seg_0727, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:25.912941: predicting seg_0732\n",
      "2025-10-02 07:36:25.946498: seg_0732, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:26.264845: predicting seg_0733\n",
      "2025-10-02 07:36:26.303457: seg_0733, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:26.616823: predicting seg_0741\n",
      "2025-10-02 07:36:26.651954: seg_0741, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-02 07:36:26.968413: predicting seg_0743\n",
      "2025-10-02 07:36:27.003268: seg_0743, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-02 07:36:27.320673: predicting seg_0746\n",
      "2025-10-02 07:36:27.357806: seg_0746, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-02 07:36:27.672191: predicting seg_0754\n",
      "2025-10-02 07:36:27.708080: seg_0754, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:28.020586: predicting seg_0757\n",
      "2025-10-02 07:36:28.053458: seg_0757, shape torch.Size([1, 116, 128, 128]), rank 0\n",
      "2025-10-02 07:36:28.368689: predicting seg_0758\n",
      "2025-10-02 07:36:28.404430: seg_0758, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:28.717377: predicting seg_0761\n",
      "2025-10-02 07:36:28.749850: seg_0761, shape torch.Size([1, 95, 128, 128]), rank 0\n",
      "2025-10-02 07:36:29.061748: predicting seg_0765\n",
      "2025-10-02 07:36:29.094959: seg_0765, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:29.410896: predicting seg_0768\n",
      "2025-10-02 07:36:29.448803: seg_0768, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:29.765383: predicting seg_0771\n",
      "2025-10-02 07:36:29.803376: seg_0771, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:30.123823: predicting seg_0776\n",
      "2025-10-02 07:36:30.153113: seg_0776, shape torch.Size([1, 98, 128, 128]), rank 0\n",
      "2025-10-02 07:36:30.467596: predicting seg_0777\n",
      "2025-10-02 07:36:30.501559: seg_0777, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-02 07:36:30.815246: predicting seg_0787\n",
      "2025-10-02 07:36:30.845874: seg_0787, shape torch.Size([1, 99, 128, 128]), rank 0\n",
      "2025-10-02 07:36:31.161318: predicting seg_0795\n",
      "2025-10-02 07:36:31.195163: seg_0795, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-02 07:36:31.508671: predicting seg_0802\n",
      "2025-10-02 07:36:31.540381: seg_0802, shape torch.Size([1, 93, 128, 128]), rank 0\n",
      "2025-10-02 07:36:31.855592: predicting seg_0804\n",
      "2025-10-02 07:36:31.889731: seg_0804, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-02 07:36:32.202410: predicting seg_0821\n",
      "2025-10-02 07:36:32.241906: seg_0821, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:32.561038: predicting seg_0826\n",
      "2025-10-02 07:36:32.599954: seg_0826, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:32.910114: predicting seg_0829\n",
      "2025-10-02 07:36:32.942982: seg_0829, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:33.258997: predicting seg_0832\n",
      "2025-10-02 07:36:33.296612: seg_0832, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:33.613048: predicting seg_0833\n",
      "2025-10-02 07:36:33.651603: seg_0833, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:33.971010: predicting seg_0838\n",
      "2025-10-02 07:36:34.008487: seg_0838, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:34.323453: predicting seg_0853\n",
      "2025-10-02 07:36:34.351611: seg_0853, shape torch.Size([1, 87, 128, 118]), rank 0\n",
      "2025-10-02 07:36:34.665105: predicting seg_0854\n",
      "2025-10-02 07:36:34.702966: seg_0854, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-02 07:36:35.020291: predicting seg_0865\n",
      "2025-10-02 07:36:35.056154: seg_0865, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:35.373660: predicting seg_0878\n",
      "2025-10-02 07:36:35.420942: seg_0878, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:35.732890: predicting seg_0883\n",
      "2025-10-02 07:36:35.768352: seg_0883, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:36.086237: predicting seg_0884\n",
      "2025-10-02 07:36:36.121987: seg_0884, shape torch.Size([1, 103, 128, 128]), rank 0\n",
      "2025-10-02 07:36:36.439713: predicting seg_0889\n",
      "2025-10-02 07:36:36.486536: seg_0889, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:36.800088: predicting seg_0890\n",
      "2025-10-02 07:36:36.835829: seg_0890, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:37.152293: predicting seg_0893\n",
      "2025-10-02 07:36:37.187197: seg_0893, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:37.503609: predicting seg_0911\n",
      "2025-10-02 07:36:37.535192: seg_0911, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:37.867816: predicting seg_0912\n",
      "2025-10-02 07:36:37.916474: seg_0912, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:38.230605: predicting seg_0913\n",
      "2025-10-02 07:36:38.262787: seg_0913, shape torch.Size([1, 109, 128, 128]), rank 0\n",
      "2025-10-02 07:36:38.574936: predicting seg_0914\n",
      "2025-10-02 07:36:38.609061: seg_0914, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:38.920544: predicting seg_0915\n",
      "2025-10-02 07:36:38.956532: seg_0915, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:39.270061: predicting seg_0925\n",
      "2025-10-02 07:36:39.304140: seg_0925, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:39.617366: predicting seg_0937\n",
      "2025-10-02 07:36:39.658062: seg_0937, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:39.968996: predicting seg_0940\n",
      "2025-10-02 07:36:40.003892: seg_0940, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:40.324153: predicting seg_0942\n",
      "2025-10-02 07:36:40.358088: seg_0942, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-02 07:36:40.672661: predicting seg_0943\n",
      "2025-10-02 07:36:40.709723: seg_0943, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:41.025368: predicting seg_0954\n",
      "2025-10-02 07:36:41.054493: seg_0954, shape torch.Size([1, 107, 128, 128]), rank 0\n",
      "2025-10-02 07:36:41.368510: predicting seg_0955\n",
      "2025-10-02 07:36:41.403581: seg_0955, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:41.721116: predicting seg_0957\n",
      "2025-10-02 07:36:41.763906: seg_0957, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:42.085977: predicting seg_0961\n",
      "2025-10-02 07:36:42.119582: seg_0961, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-02 07:36:42.435521: predicting seg_0962\n",
      "2025-10-02 07:36:42.470615: seg_0962, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:42.785752: predicting seg_0966\n",
      "2025-10-02 07:36:42.820528: seg_0966, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:43.133912: predicting seg_0967\n",
      "2025-10-02 07:36:43.172945: seg_0967, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:43.486959: predicting seg_0974\n",
      "2025-10-02 07:36:43.525487: seg_0974, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:43.843309: predicting seg_0977\n",
      "2025-10-02 07:36:43.880917: seg_0977, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:44.192878: predicting seg_0979\n",
      "2025-10-02 07:36:44.227680: seg_0979, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:44.545972: predicting seg_0981\n",
      "2025-10-02 07:36:44.583782: seg_0981, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-02 07:36:44.898321: predicting seg_0982\n",
      "2025-10-02 07:36:44.932034: seg_0982, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:45.247003: predicting seg_0997\n",
      "2025-10-02 07:36:45.277233: seg_0997, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-02 07:36:59.925719: Validation complete\n",
      "2025-10-02 07:36:59.926144: Mean Validation Dice:  0.9825405044170944\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 設定 conda gcc 路徑\n",
    "os.environ['CC'] = \"/home/sandy0317/.conda/envs/nnunet/bin/x86_64-conda-linux-gnu-gcc\" #改成自己的帳號名稱放置路徑中\n",
    "os.environ['CXX'] = \"/home/sandy0317/.conda/envs/nnunet/bin/x86_64-conda-linux-gnu-g++\" #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "# 設定 nnU-Net 資料夾\n",
    "os.environ['nnUNet_raw'] = 'practice_nnunet/nnUNet_raw'\n",
    "os.environ['nnUNet_preprocessed'] = 'practice_nnunet/nnUNet_preprocessed'\n",
    "os.environ['nnUNet_results'] = 'practice_nnunet/nnUNet_results'\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_train Dataset001 3d_fullres 0 -tr nnUNetTrainer_10epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc999760-c288-4fe0-84b8-2bcd27ed8474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 3 cases in the source folder\n",
      "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 3 cases that I would like to predict\n",
      "\n",
      "Predicting sample_1:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_1\n",
      "\n",
      "Predicting sample_2:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_2\n",
      "\n",
      "Predicting sample_3:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.18it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_3\n",
      "推論完成，結果存於： nnUNet_predictions\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 6. 推論 (Inference)\n",
    "# ===========================\n",
    "import os\n",
    "\n",
    "nnUNet_raw = 'practice_nnunet/nnUNet_raw'\n",
    "\n",
    "# 測試影像資料夾\n",
    "input_folder = os.path.join(nnUNet_raw, \"Dataset001\", \"imagesTs\")\n",
    "\n",
    "# 改成相對路徑或家目錄下\n",
    "output_folder = 'nnUNet_predictions'  # 或者 os.path.expanduser('~/nnUNet_predictions')\n",
    "\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_predict -d 1 -i $input_folder -o $output_folder -tr nnUNetTrainer_10epochs -c 3d_fullres -f 0\n",
    "\n",
    "print(\"推論完成，結果存於：\", output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c1ac2-ef6c-4272-8739-aa8864321c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 7. 驗證 (可選)\n",
    "# ===========================\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_evaluate_folder -ref $nnUNet_raw/Dataset001/labelsTr -pred $output_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c3115-db8b-4387-a92c-e04bd855de43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa752444-d60f-4013-b606-7228d0c3b7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnU-Net)",
   "language": "python",
   "name": "nnunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

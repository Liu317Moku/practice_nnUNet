{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941bf0b2-bd0d-4ceb-a15f-e0315f2be862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnU-Net 資料夾已建立在 practice_nnunet/ 下\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 設定 nnU-Net 資料夾路徑\n",
    "base_dir = \"practice_nnunet\"\n",
    "nnUNet_raw = os.path.join(base_dir, \"nnUNet_raw\")\n",
    "nnUNet_preprocessed = os.path.join(base_dir, \"nnUNet_preprocessed\")\n",
    "nnUNet_results = os.path.join(base_dir, \"nnUNet_results\")\n",
    "\n",
    "# 建立資料夾\n",
    "os.makedirs(nnUNet_raw, exist_ok=True)\n",
    "os.makedirs(nnUNet_preprocessed, exist_ok=True)\n",
    "os.makedirs(nnUNet_results, exist_ok=True)\n",
    "\n",
    "# 設定環境變數\n",
    "os.environ[\"nnUNet_raw\"] = nnUNet_raw\n",
    "os.environ[\"nnUNet_preprocessed\"] = nnUNet_preprocessed\n",
    "os.environ[\"nnUNet_results\"] = nnUNet_results\n",
    "\n",
    "print(\"nnU-Net 資料夾已建立在 practice_nnunet/ 下\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d8e0cb-854c-409c-ad4b-5fe1e2956539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# -------------------------------\n",
    "# nnU-Net 資料夾\n",
    "# -------------------------------\n",
    "task_dir = \"/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw/Dataset001\" #改成自己的帳號名稱放置路徑中\n",
    "imagesTr_dir = os.path.join(task_dir, \"imagesTr\")\n",
    "labelsTr_dir = os.path.join(task_dir, \"labelsTr\")\n",
    "imagesTs_dir = os.path.join(task_dir, \"imagesTs\")\n",
    "os.makedirs(imagesTr_dir, exist_ok=True)\n",
    "os.makedirs(labelsTr_dir, exist_ok=True)\n",
    "os.makedirs(imagesTs_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 原始影像與 mask 資料夾\n",
    "# -------------------------------\n",
    "src_images = \"/home/sandy0317/Public/train/med-ddpm-1/image\" #改成自己的帳號名稱放置路徑中\n",
    "src_masks = \"/home/sandy0317/Public/train/med-ddpm-1/mask\" #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "# -------------------------------\n",
    "# 找出所有影像檔\n",
    "# -------------------------------\n",
    "image_paths = sorted(glob.glob(os.path.join(src_images, \"*\")))\n",
    "mask_paths = sorted(glob.glob(os.path.join(src_masks, \"*\")))\n",
    "\n",
    "# -------------------------------\n",
    "# 檢查數量一致\n",
    "# -------------------------------\n",
    "assert len(image_paths) == len(mask_paths), f\"影像數量({len(image_paths)})與標註數量({len(mask_paths)})不一致！\"\n",
    "\n",
    "# -------------------------------\n",
    "# 複製並重新命名\n",
    "# -------------------------------\n",
    "for idx, (img_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
    "    case_id = f\"seg_{idx:04d}\"\n",
    "\n",
    "    # nnU-Net 格式檔名\n",
    "    new_img_name = f\"{case_id}_0000.nii.gz\"  # 影像加 _0000\n",
    "    new_mask_name = f\"{case_id}.nii.gz\"      # mask 保留 case_id\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(imagesTr_dir, new_img_name))\n",
    "    shutil.copy(mask_path, os.path.join(labelsTr_dir, new_mask_name))\n",
    "\n",
    "print(f\"✅ 已完成轉換並複製 {len(image_paths)} 筆資料到 nnU-Net 格式資料夾：{task_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9a2dd9-7f9c-400b-b22d-d0824594c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 標籤檔已修正完成 (2→1)\n",
      "✅ dataset.json 已生成，背景標籤正確\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed\n",
      "✅ 資料夾確認: /home/sandy0317/practice_nnunet/nnUNet_results\n",
      "🚀 開始 nnU-Net 資料集預處理...\n",
      "Fingerprint extraction...\n",
      "Dataset001\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:59<00:00, 16.87it/s]\n",
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Dropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [127. 128. 128.], 3d_lowres: [127, 128, 128]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 199, 'patch_size': (np.int64(128), np.int64(128)), 'median_image_size_in_voxels': array([128., 128.]), 'spacing': array([1.5, 1.5]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(128), np.int64(128), np.int64(128)), 'median_image_size_in_voxels': array([127., 128., 128.]), 'spacing': array([1.5, 1.5, 1.5]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n",
      "\n",
      "Plans were saved to /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed/Dataset001/nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset001\n",
      "Configuration: 2d...\n",
      "100%|███████████████████████████████████████| 1000/1000 [14:28<00:00,  1.15it/s]\n",
      "Configuration: 3d_fullres...\n",
      "100%|███████████████████████████████████████| 1000/1000 [22:55<00:00,  1.38s/it]\n",
      "Configuration: 3d_lowres...\n",
      "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset001. Skipping.\n",
      "✅ nnU-Net 資料集預處理完成\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# nnU-Net Dataset 前置流程 (Jupyter Notebook)\n",
    "# ===============================\n",
    "#執行前請先至Terminal確認自己的nnU-Net可執行檔的位置，如語法which nnUNetv2_plan_and_preprocess\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ 原始資料集路徑\n",
    "# -------------------------------\n",
    "task_dir = \"/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw/Dataset001\" #改成自己的帳號名稱放置路徑中\n",
    "imagesTr_dir = os.path.join(task_dir, \"imagesTr\")\n",
    "labelsTr_dir = os.path.join(task_dir, \"labelsTr\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ 自動修正標籤檔：將 2 映射成 1\n",
    "# -------------------------------\n",
    "label_files = sorted(glob.glob(os.path.join(labelsTr_dir, \"*.nii.gz\")))\n",
    "for f in label_files:\n",
    "    img = nib.load(f)\n",
    "    data = img.get_fdata()\n",
    "    data[data == 2] = 1\n",
    "    nib.Nifti1Image(data.astype(np.uint8), img.affine, img.header).to_filename(f)\n",
    "print(\"✅ 標籤檔已修正完成 (2→1)\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ 生成 dataset.json\n",
    "# -------------------------------\n",
    "image_files = sorted(glob.glob(os.path.join(imagesTr_dir, \"*.nii.gz\")))\n",
    "\n",
    "dataset_json = {\n",
    "    \"name\": \"Dataset001\",\n",
    "    \"description\": \"Segmentation\",\n",
    "    \"tensorImageSize\": \"3D\",\n",
    "    \"modality\": {\"0\": \"CT\"},\n",
    "    \"labels\": {\"background\": 0, \"seg\": 1},  # ✅ 修正\n",
    "    \"numTraining\": len(image_files),\n",
    "    \"numTest\": 0,\n",
    "    \"file_ending\": \".nii.gz\",\n",
    "    \"channel_names\": {\"0\": \"CT\"},\n",
    "    \"training\": [\n",
    "        {\n",
    "            \"image\": os.path.join(\"imagesTr\", os.path.basename(img)),\n",
    "            \"label\": os.path.join(\"labelsTr\", os.path.basename(lbl))\n",
    "        }\n",
    "        for img, lbl in zip(image_files, label_files)\n",
    "    ],\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "\n",
    "dataset_json_path = os.path.join(task_dir, \"dataset.json\")\n",
    "with open(dataset_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_json, f, indent=4, ensure_ascii=False)\n",
    "print(\"✅ dataset.json 已生成，背景標籤正確\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ 設定 nnU-Net 環境變數\n",
    "# -------------------------------\n",
    "nnunet_raw = \"/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw\" #改成自己的帳號名稱放置路徑中\n",
    "nnunet_preprocessed = \"/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed\" #改成自己的帳號名稱放置路徑中\n",
    "nnunet_results = \"/home/sandy0317/practice_nnunet/nnUNet_results\" #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "os.environ[\"nnUNet_raw\"] = nnunet_raw\n",
    "os.environ[\"nnUNet_preprocessed\"] = nnunet_preprocessed\n",
    "os.environ[\"RESULTS_FOLDER\"] = nnunet_results\n",
    "os.environ[\"nnUNet_results\"] = nnunet_results  # ✅ 必須\n",
    "\n",
    "# 建立資料夾\n",
    "for p in [nnunet_raw, nnunet_preprocessed, nnunet_results]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    print(f\"✅ 資料夾確認: {p}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ 執行 nnU-Net 資料集預處理 (CLI)\n",
    "# -------------------------------\n",
    "\n",
    "cmd = [\n",
    "    \"/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_plan_and_preprocess\", #改成自己的帳號名稱放置路徑中\n",
    "    \"-p\", task_dir,            # 指定 Task 資料夾完整路徑\n",
    "    \"--verify_dataset_integrity\"\n",
    "]\n",
    "\n",
    "print(\"🚀 開始 nnU-Net 資料集預處理...\")\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "#!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity\n",
    "!/home/sandy0317/.local/bin/nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity\n",
    "print(\"✅ nnU-Net 資料集預處理完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5958ce6e-51d2-4c8c-acfa-97472e6b107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-10-03 08:43:09.389088: Using torch.compile...\n",
      "2025-10-03 08:43:11.842631: do_dummy_2d_data_aug: False\n",
      "2025-10-03 08:43:11.848524: Creating new 5-fold cross-validation split...\n",
      "2025-10-03 08:43:11.866832: Desired fold for training: 0\n",
      "2025-10-03 08:43:11.867006: This split has 800 training and 200 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [127.0, 128.0, 128.0], 'spacing': [1.5, 1.5, 1.5], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 1.5, 1.5], 'original_median_shape_after_transp': [127, 128, 128], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0, 'mean': 0.17309385538101196, 'median': 0.23840078711509705, 'min': -1.0, 'percentile_00_5': -0.9529684787988663, 'percentile_99_5': 0.8862527015805242, 'std': 0.36723610758781433}}} \n",
      "\n",
      "2025-10-03 08:43:14.182974: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-10-03 08:43:14.213908: \n",
      "2025-10-03 08:43:14.214951: Epoch 0\n",
      "2025-10-03 08:43:14.215526: Current learning rate: 0.01\n",
      "2025-10-03 08:45:11.277955: train_loss -0.5687\n",
      "2025-10-03 08:45:11.279167: val_loss -0.8147\n",
      "2025-10-03 08:45:11.279419: Pseudo dice [np.float32(0.958)]\n",
      "2025-10-03 08:45:11.279660: Epoch time: 117.07 s\n",
      "2025-10-03 08:45:11.279835: Yayy! New best EMA pseudo Dice: 0.9580000042915344\n",
      "2025-10-03 08:45:14.966192: \n",
      "2025-10-03 08:45:14.966679: Epoch 1\n",
      "2025-10-03 08:45:14.967135: Current learning rate: 0.0091\n",
      "2025-10-03 08:46:17.517369: train_loss -0.7969\n",
      "2025-10-03 08:46:17.518412: val_loss -0.8714\n",
      "2025-10-03 08:46:17.518641: Pseudo dice [np.float32(0.9688)]\n",
      "2025-10-03 08:46:17.518869: Epoch time: 62.55 s\n",
      "2025-10-03 08:46:17.519047: Yayy! New best EMA pseudo Dice: 0.9591000080108643\n",
      "2025-10-03 08:46:22.559747: \n",
      "2025-10-03 08:46:22.560534: Epoch 2\n",
      "2025-10-03 08:46:22.560817: Current learning rate: 0.00818\n",
      "2025-10-03 08:47:25.189412: train_loss -0.8582\n",
      "2025-10-03 08:47:25.190556: val_loss -0.8865\n",
      "2025-10-03 08:47:25.190781: Pseudo dice [np.float32(0.9743)]\n",
      "2025-10-03 08:47:25.191001: Epoch time: 62.63 s\n",
      "2025-10-03 08:47:25.191177: Yayy! New best EMA pseudo Dice: 0.9606000185012817\n",
      "2025-10-03 08:47:29.702653: \n",
      "2025-10-03 08:47:29.703500: Epoch 3\n",
      "2025-10-03 08:47:29.703787: Current learning rate: 0.00725\n",
      "2025-10-03 08:48:32.372291: train_loss -0.8755\n",
      "2025-10-03 08:48:32.373153: val_loss -0.8964\n",
      "2025-10-03 08:48:32.373392: Pseudo dice [np.float32(0.9754)]\n",
      "2025-10-03 08:48:32.373648: Epoch time: 62.67 s\n",
      "2025-10-03 08:48:32.373826: Yayy! New best EMA pseudo Dice: 0.9621000289916992\n",
      "2025-10-03 08:48:36.780907: \n",
      "2025-10-03 08:48:36.781652: Epoch 4\n",
      "2025-10-03 08:48:36.781929: Current learning rate: 0.00631\n",
      "2025-10-03 08:49:39.277308: train_loss -0.8997\n",
      "2025-10-03 08:49:39.278110: val_loss -0.9151\n",
      "2025-10-03 08:49:39.278344: Pseudo dice [np.float32(0.9792)]\n",
      "2025-10-03 08:49:39.278565: Epoch time: 62.5 s\n",
      "2025-10-03 08:49:39.278764: Yayy! New best EMA pseudo Dice: 0.9638000130653381\n",
      "2025-10-03 08:49:43.721271: \n",
      "2025-10-03 08:49:43.722044: Epoch 5\n",
      "2025-10-03 08:49:43.722378: Current learning rate: 0.00536\n",
      "2025-10-03 08:50:46.248482: train_loss -0.908\n",
      "2025-10-03 08:50:46.249346: val_loss -0.9196\n",
      "2025-10-03 08:50:46.249568: Pseudo dice [np.float32(0.9809)]\n",
      "2025-10-03 08:50:46.249791: Epoch time: 62.53 s\n",
      "2025-10-03 08:50:46.249973: Yayy! New best EMA pseudo Dice: 0.965499997138977\n",
      "2025-10-03 08:50:50.535065: \n",
      "2025-10-03 08:50:50.535868: Epoch 6\n",
      "2025-10-03 08:50:50.536141: Current learning rate: 0.00438\n",
      "2025-10-03 08:51:53.066995: train_loss -0.9117\n",
      "2025-10-03 08:51:53.067794: val_loss -0.9194\n",
      "2025-10-03 08:51:53.068009: Pseudo dice [np.float32(0.9805)]\n",
      "2025-10-03 08:51:53.068228: Epoch time: 62.54 s\n",
      "2025-10-03 08:51:53.068423: Yayy! New best EMA pseudo Dice: 0.9670000076293945\n",
      "2025-10-03 08:51:57.368809: \n",
      "2025-10-03 08:51:57.369507: Epoch 7\n",
      "2025-10-03 08:51:57.369761: Current learning rate: 0.00338\n",
      "2025-10-03 08:52:59.886332: train_loss -0.9168\n",
      "2025-10-03 08:52:59.887057: val_loss -0.9277\n",
      "2025-10-03 08:52:59.887340: Pseudo dice [np.float32(0.9825)]\n",
      "2025-10-03 08:52:59.887632: Epoch time: 62.52 s\n",
      "2025-10-03 08:52:59.887881: Yayy! New best EMA pseudo Dice: 0.9685999751091003\n",
      "2025-10-03 08:53:04.257775: \n",
      "2025-10-03 08:53:04.258534: Epoch 8\n",
      "2025-10-03 08:53:04.258867: Current learning rate: 0.00235\n",
      "2025-10-03 08:54:07.008382: train_loss -0.9182\n",
      "2025-10-03 08:54:07.009078: val_loss -0.9258\n",
      "2025-10-03 08:54:07.009331: Pseudo dice [np.float32(0.9821)]\n",
      "2025-10-03 08:54:07.009552: Epoch time: 62.75 s\n",
      "2025-10-03 08:54:07.009724: Yayy! New best EMA pseudo Dice: 0.9699000120162964\n",
      "2025-10-03 08:54:11.383878: \n",
      "2025-10-03 08:54:11.384636: Epoch 9\n",
      "2025-10-03 08:54:11.384907: Current learning rate: 0.00126\n",
      "2025-10-03 08:55:14.082749: train_loss -0.922\n",
      "2025-10-03 08:55:14.083553: val_loss -0.929\n",
      "2025-10-03 08:55:14.083767: Pseudo dice [np.float32(0.9827)]\n",
      "2025-10-03 08:55:14.083992: Epoch time: 62.7 s\n",
      "2025-10-03 08:55:14.084202: Yayy! New best EMA pseudo Dice: 0.9711999893188477\n",
      "2025-10-03 08:55:18.941951: Training done.\n",
      "2025-10-03 08:55:18.988006: Using splits from existing split file: /home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed/Dataset001/splits_final.json\n",
      "2025-10-03 08:55:18.991490: The split file contains 5 splits.\n",
      "2025-10-03 08:55:18.991748: Desired fold for training: 0\n",
      "2025-10-03 08:55:18.991935: This split has 800 training and 200 validation cases.\n",
      "2025-10-03 08:55:19.006156: predicting seg_0000\n",
      "2025-10-03 08:55:19.060284: seg_0000, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:41.857679: predicting seg_0003\n",
      "2025-10-03 08:55:41.902659: seg_0003, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:42.237677: predicting seg_0004\n",
      "2025-10-03 08:55:42.277446: seg_0004, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:42.587531: predicting seg_0010\n",
      "2025-10-03 08:55:42.631875: seg_0010, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:42.962542: predicting seg_0017\n",
      "2025-10-03 08:55:43.000645: seg_0017, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:43.321523: predicting seg_0033\n",
      "2025-10-03 08:55:43.356819: seg_0033, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:43.669780: predicting seg_0041\n",
      "2025-10-03 08:55:43.720795: seg_0041, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:44.039165: predicting seg_0047\n",
      "2025-10-03 08:55:44.084915: seg_0047, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:44.410579: predicting seg_0053\n",
      "2025-10-03 08:55:44.451100: seg_0053, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:44.761655: predicting seg_0056\n",
      "2025-10-03 08:55:44.803956: seg_0056, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:45.115083: predicting seg_0060\n",
      "2025-10-03 08:55:45.160022: seg_0060, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:45.470601: predicting seg_0065\n",
      "2025-10-03 08:55:45.515365: seg_0065, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:45.825836: predicting seg_0073\n",
      "2025-10-03 08:55:45.897110: seg_0073, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:46.211922: predicting seg_0090\n",
      "2025-10-03 08:55:46.249976: seg_0090, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:46.561209: predicting seg_0101\n",
      "2025-10-03 08:55:46.595915: seg_0101, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:46.907899: predicting seg_0104\n",
      "2025-10-03 08:55:46.952093: seg_0104, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:47.267716: predicting seg_0106\n",
      "2025-10-03 08:55:47.311883: seg_0106, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:47.626152: predicting seg_0109\n",
      "2025-10-03 08:55:47.664687: seg_0109, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:47.988421: predicting seg_0111\n",
      "2025-10-03 08:55:48.026810: seg_0111, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:48.337022: predicting seg_0115\n",
      "2025-10-03 08:55:48.371765: seg_0115, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:48.694921: predicting seg_0117\n",
      "2025-10-03 08:55:48.726828: seg_0117, shape torch.Size([1, 89, 128, 121]), rank 0\n",
      "2025-10-03 08:55:49.041115: predicting seg_0127\n",
      "2025-10-03 08:55:49.080373: seg_0127, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:49.392796: predicting seg_0130\n",
      "2025-10-03 08:55:49.429602: seg_0130, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:49.741862: predicting seg_0133\n",
      "2025-10-03 08:55:49.778625: seg_0133, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:50.094735: predicting seg_0135\n",
      "2025-10-03 08:55:50.131802: seg_0135, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:50.444988: predicting seg_0136\n",
      "2025-10-03 08:55:50.488297: seg_0136, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:50.805737: predicting seg_0138\n",
      "2025-10-03 08:55:50.864469: seg_0138, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:51.188082: predicting seg_0144\n",
      "2025-10-03 08:55:51.222946: seg_0144, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:51.536988: predicting seg_0150\n",
      "2025-10-03 08:55:51.571237: seg_0150, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:51.886567: predicting seg_0153\n",
      "2025-10-03 08:55:51.923002: seg_0153, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:52.242358: predicting seg_0156\n",
      "2025-10-03 08:55:52.297840: seg_0156, shape torch.Size([1, 123, 128, 128]), rank 0\n",
      "2025-10-03 08:55:52.618297: predicting seg_0158\n",
      "2025-10-03 08:55:52.657062: seg_0158, shape torch.Size([1, 101, 128, 128]), rank 0\n",
      "2025-10-03 08:55:52.973369: predicting seg_0162\n",
      "2025-10-03 08:55:53.009829: seg_0162, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:53.321944: predicting seg_0164\n",
      "2025-10-03 08:55:53.358491: seg_0164, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:53.668669: predicting seg_0168\n",
      "2025-10-03 08:55:53.704587: seg_0168, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:54.015645: predicting seg_0173\n",
      "2025-10-03 08:55:54.054014: seg_0173, shape torch.Size([1, 118, 128, 121]), rank 0\n",
      "2025-10-03 08:55:54.370544: predicting seg_0181\n",
      "2025-10-03 08:55:54.406740: seg_0181, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:54.717964: predicting seg_0186\n",
      "2025-10-03 08:55:54.756388: seg_0186, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:55.071071: predicting seg_0187\n",
      "2025-10-03 08:55:55.115974: seg_0187, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:55.429238: predicting seg_0190\n",
      "2025-10-03 08:55:55.464717: seg_0190, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:55.773219: predicting seg_0196\n",
      "2025-10-03 08:55:55.820658: seg_0196, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:56.138038: predicting seg_0198\n",
      "2025-10-03 08:55:56.172714: seg_0198, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:56.497706: predicting seg_0199\n",
      "2025-10-03 08:55:56.531091: seg_0199, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:56.842521: predicting seg_0203\n",
      "2025-10-03 08:55:56.878463: seg_0203, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:55:57.190610: predicting seg_0207\n",
      "2025-10-03 08:55:57.229138: seg_0207, shape torch.Size([1, 123, 128, 128]), rank 0\n",
      "2025-10-03 08:55:57.544286: predicting seg_0216\n",
      "2025-10-03 08:55:57.581037: seg_0216, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-03 08:55:57.893432: predicting seg_0220\n",
      "2025-10-03 08:55:57.930090: seg_0220, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-03 08:55:58.245844: predicting seg_0222\n",
      "2025-10-03 08:55:58.273704: seg_0222, shape torch.Size([1, 95, 128, 125]), rank 0\n",
      "2025-10-03 08:55:58.583550: predicting seg_0238\n",
      "2025-10-03 08:55:58.619865: seg_0238, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:58.933299: predicting seg_0239\n",
      "2025-10-03 08:55:58.970217: seg_0239, shape torch.Size([1, 120, 128, 125]), rank 0\n",
      "2025-10-03 08:55:59.283281: predicting seg_0244\n",
      "2025-10-03 08:55:59.316667: seg_0244, shape torch.Size([1, 114, 128, 120]), rank 0\n",
      "2025-10-03 08:55:59.634686: predicting seg_0250\n",
      "2025-10-03 08:55:59.674347: seg_0250, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:55:59.988062: predicting seg_0254\n",
      "2025-10-03 08:56:00.023540: seg_0254, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:00.337590: predicting seg_0258\n",
      "2025-10-03 08:56:00.373899: seg_0258, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:00.688933: predicting seg_0262\n",
      "2025-10-03 08:56:00.727953: seg_0262, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:01.044546: predicting seg_0266\n",
      "2025-10-03 08:56:01.083986: seg_0266, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:01.398859: predicting seg_0268\n",
      "2025-10-03 08:56:01.441893: seg_0268, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:01.751842: predicting seg_0270\n",
      "2025-10-03 08:56:01.785759: seg_0270, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:02.096199: predicting seg_0280\n",
      "2025-10-03 08:56:02.131396: seg_0280, shape torch.Size([1, 110, 128, 117]), rank 0\n",
      "2025-10-03 08:56:02.452074: predicting seg_0281\n",
      "2025-10-03 08:56:02.496917: seg_0281, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:02.814595: predicting seg_0284\n",
      "2025-10-03 08:56:02.854507: seg_0284, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:03.168933: predicting seg_0286\n",
      "2025-10-03 08:56:03.198866: seg_0286, shape torch.Size([1, 78, 128, 128]), rank 0\n",
      "2025-10-03 08:56:03.508709: predicting seg_0296\n",
      "2025-10-03 08:56:03.552858: seg_0296, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-03 08:56:03.866213: predicting seg_0298\n",
      "2025-10-03 08:56:03.898960: seg_0298, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:04.208855: predicting seg_0304\n",
      "2025-10-03 08:56:04.248621: seg_0304, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:04.561045: predicting seg_0308\n",
      "2025-10-03 08:56:04.600336: seg_0308, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:04.913782: predicting seg_0311\n",
      "2025-10-03 08:56:04.952781: seg_0311, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:05.265564: predicting seg_0314\n",
      "2025-10-03 08:56:05.300078: seg_0314, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:05.614457: predicting seg_0320\n",
      "2025-10-03 08:56:05.654492: seg_0320, shape torch.Size([1, 127, 128, 124]), rank 0\n",
      "2025-10-03 08:56:05.971725: predicting seg_0322\n",
      "2025-10-03 08:56:06.007710: seg_0322, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-03 08:56:06.351873: predicting seg_0326\n",
      "2025-10-03 08:56:06.416240: seg_0326, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:06.750498: predicting seg_0327\n",
      "2025-10-03 08:56:06.789865: seg_0327, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:07.125472: predicting seg_0328\n",
      "2025-10-03 08:56:07.162626: seg_0328, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:07.473816: predicting seg_0332\n",
      "2025-10-03 08:56:07.513816: seg_0332, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:07.824485: predicting seg_0334\n",
      "2025-10-03 08:56:07.863372: seg_0334, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:08.178695: predicting seg_0343\n",
      "2025-10-03 08:56:08.212079: seg_0343, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-03 08:56:08.529847: predicting seg_0346\n",
      "2025-10-03 08:56:08.572291: seg_0346, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:08.885012: predicting seg_0352\n",
      "2025-10-03 08:56:08.924042: seg_0352, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:09.250961: predicting seg_0358\n",
      "2025-10-03 08:56:09.321750: seg_0358, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-03 08:56:09.667225: predicting seg_0370\n",
      "2025-10-03 08:56:09.709161: seg_0370, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-03 08:56:10.029800: predicting seg_0377\n",
      "2025-10-03 08:56:10.087471: seg_0377, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:10.407400: predicting seg_0380\n",
      "2025-10-03 08:56:10.444694: seg_0380, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:10.764987: predicting seg_0391\n",
      "2025-10-03 08:56:10.811396: seg_0391, shape torch.Size([1, 120, 128, 124]), rank 0\n",
      "2025-10-03 08:56:11.139424: predicting seg_0400\n",
      "2025-10-03 08:56:11.179930: seg_0400, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:11.491944: predicting seg_0404\n",
      "2025-10-03 08:56:11.536101: seg_0404, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:11.848472: predicting seg_0413\n",
      "2025-10-03 08:56:11.881663: seg_0413, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:12.197800: predicting seg_0422\n",
      "2025-10-03 08:56:12.237435: seg_0422, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:12.551654: predicting seg_0430\n",
      "2025-10-03 08:56:12.589646: seg_0430, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:12.904384: predicting seg_0435\n",
      "2025-10-03 08:56:12.941347: seg_0435, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:13.258858: predicting seg_0437\n",
      "2025-10-03 08:56:13.296366: seg_0437, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:13.610117: predicting seg_0440\n",
      "2025-10-03 08:56:13.648590: seg_0440, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:13.962062: predicting seg_0454\n",
      "2025-10-03 08:56:13.994185: seg_0454, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:14.306638: predicting seg_0458\n",
      "2025-10-03 08:56:14.337967: seg_0458, shape torch.Size([1, 87, 128, 128]), rank 0\n",
      "2025-10-03 08:56:14.648644: predicting seg_0462\n",
      "2025-10-03 08:56:14.684867: seg_0462, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:14.998806: predicting seg_0468\n",
      "2025-10-03 08:56:15.034477: seg_0468, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:15.345677: predicting seg_0469\n",
      "2025-10-03 08:56:15.379124: seg_0469, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:15.695583: predicting seg_0473\n",
      "2025-10-03 08:56:15.730226: seg_0473, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:16.043308: predicting seg_0474\n",
      "2025-10-03 08:56:16.076831: seg_0474, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:16.388574: predicting seg_0479\n",
      "2025-10-03 08:56:16.425548: seg_0479, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:16.737154: predicting seg_0481\n",
      "2025-10-03 08:56:16.771812: seg_0481, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:17.085009: predicting seg_0484\n",
      "2025-10-03 08:56:17.117980: seg_0484, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:17.432104: predicting seg_0488\n",
      "2025-10-03 08:56:17.468704: seg_0488, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:17.779489: predicting seg_0493\n",
      "2025-10-03 08:56:17.821365: seg_0493, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:18.134013: predicting seg_0503\n",
      "2025-10-03 08:56:18.172687: seg_0503, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-03 08:56:18.486647: predicting seg_0504\n",
      "2025-10-03 08:56:18.523590: seg_0504, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:18.836392: predicting seg_0509\n",
      "2025-10-03 08:56:18.875383: seg_0509, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:19.188095: predicting seg_0516\n",
      "2025-10-03 08:56:19.222619: seg_0516, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:19.533782: predicting seg_0517\n",
      "2025-10-03 08:56:19.577271: seg_0517, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:19.905234: predicting seg_0521\n",
      "2025-10-03 08:56:19.945817: seg_0521, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:20.270872: predicting seg_0540\n",
      "2025-10-03 08:56:20.306309: seg_0540, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:20.625875: predicting seg_0545\n",
      "2025-10-03 08:56:20.662230: seg_0545, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:20.974819: predicting seg_0551\n",
      "2025-10-03 08:56:21.007971: seg_0551, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:21.322292: predicting seg_0558\n",
      "2025-10-03 08:56:21.359436: seg_0558, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-03 08:56:21.674381: predicting seg_0560\n",
      "2025-10-03 08:56:21.710113: seg_0560, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:22.022929: predicting seg_0566\n",
      "2025-10-03 08:56:22.060216: seg_0566, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:22.372841: predicting seg_0569\n",
      "2025-10-03 08:56:22.416690: seg_0569, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:22.729374: predicting seg_0578\n",
      "2025-10-03 08:56:22.769570: seg_0578, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:23.086728: predicting seg_0582\n",
      "2025-10-03 08:56:23.121501: seg_0582, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:23.435718: predicting seg_0589\n",
      "2025-10-03 08:56:23.471053: seg_0589, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:23.780663: predicting seg_0590\n",
      "2025-10-03 08:56:23.819646: seg_0590, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:24.132964: predicting seg_0593\n",
      "2025-10-03 08:56:24.168738: seg_0593, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:24.482705: predicting seg_0599\n",
      "2025-10-03 08:56:24.525476: seg_0599, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:24.842134: predicting seg_0603\n",
      "2025-10-03 08:56:24.872087: seg_0603, shape torch.Size([1, 93, 128, 128]), rank 0\n",
      "2025-10-03 08:56:25.185278: predicting seg_0604\n",
      "2025-10-03 08:56:25.223198: seg_0604, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:25.536127: predicting seg_0606\n",
      "2025-10-03 08:56:25.569013: seg_0606, shape torch.Size([1, 109, 128, 128]), rank 0\n",
      "2025-10-03 08:56:25.884237: predicting seg_0609\n",
      "2025-10-03 08:56:25.919587: seg_0609, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:26.229306: predicting seg_0613\n",
      "2025-10-03 08:56:26.264094: seg_0613, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:26.577645: predicting seg_0617\n",
      "2025-10-03 08:56:26.614531: seg_0617, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:26.928296: predicting seg_0624\n",
      "2025-10-03 08:56:26.963145: seg_0624, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:27.277043: predicting seg_0625\n",
      "2025-10-03 08:56:27.314377: seg_0625, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:27.630884: predicting seg_0634\n",
      "2025-10-03 08:56:27.674296: seg_0634, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:27.984957: predicting seg_0656\n",
      "2025-10-03 08:56:28.030367: seg_0656, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:28.343681: predicting seg_0663\n",
      "2025-10-03 08:56:28.381274: seg_0663, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:28.699089: predicting seg_0664\n",
      "2025-10-03 08:56:28.743853: seg_0664, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:29.069445: predicting seg_0671\n",
      "2025-10-03 08:56:29.110755: seg_0671, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:29.428686: predicting seg_0677\n",
      "2025-10-03 08:56:29.462551: seg_0677, shape torch.Size([1, 125, 128, 128]), rank 0\n",
      "2025-10-03 08:56:29.779978: predicting seg_0680\n",
      "2025-10-03 08:56:29.815954: seg_0680, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:30.132820: predicting seg_0696\n",
      "2025-10-03 08:56:30.171235: seg_0696, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:30.485171: predicting seg_0700\n",
      "2025-10-03 08:56:30.517757: seg_0700, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:30.843081: predicting seg_0703\n",
      "2025-10-03 08:56:30.884089: seg_0703, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:31.199482: predicting seg_0719\n",
      "2025-10-03 08:56:31.243499: seg_0719, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:31.564397: predicting seg_0725\n",
      "2025-10-03 08:56:31.609958: seg_0725, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:31.927492: predicting seg_0726\n",
      "2025-10-03 08:56:31.966281: seg_0726, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:32.278747: predicting seg_0727\n",
      "2025-10-03 08:56:32.315539: seg_0727, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:32.631958: predicting seg_0732\n",
      "2025-10-03 08:56:32.665646: seg_0732, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:32.981067: predicting seg_0733\n",
      "2025-10-03 08:56:33.025582: seg_0733, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:33.342635: predicting seg_0741\n",
      "2025-10-03 08:56:33.377936: seg_0741, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-03 08:56:33.692849: predicting seg_0743\n",
      "2025-10-03 08:56:33.727926: seg_0743, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-03 08:56:34.041423: predicting seg_0746\n",
      "2025-10-03 08:56:34.077658: seg_0746, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-03 08:56:34.400113: predicting seg_0754\n",
      "2025-10-03 08:56:34.437624: seg_0754, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:34.750669: predicting seg_0757\n",
      "2025-10-03 08:56:34.784313: seg_0757, shape torch.Size([1, 116, 128, 128]), rank 0\n",
      "2025-10-03 08:56:35.097509: predicting seg_0758\n",
      "2025-10-03 08:56:35.132771: seg_0758, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:35.448610: predicting seg_0761\n",
      "2025-10-03 08:56:35.482017: seg_0761, shape torch.Size([1, 95, 128, 128]), rank 0\n",
      "2025-10-03 08:56:35.793991: predicting seg_0765\n",
      "2025-10-03 08:56:35.827891: seg_0765, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:36.142659: predicting seg_0768\n",
      "2025-10-03 08:56:36.181010: seg_0768, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:36.496276: predicting seg_0771\n",
      "2025-10-03 08:56:36.533924: seg_0771, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:36.848852: predicting seg_0776\n",
      "2025-10-03 08:56:36.879189: seg_0776, shape torch.Size([1, 98, 128, 128]), rank 0\n",
      "2025-10-03 08:56:37.193587: predicting seg_0777\n",
      "2025-10-03 08:56:37.227282: seg_0777, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-03 08:56:37.542940: predicting seg_0787\n",
      "2025-10-03 08:56:37.574534: seg_0787, shape torch.Size([1, 99, 128, 128]), rank 0\n",
      "2025-10-03 08:56:37.885940: predicting seg_0795\n",
      "2025-10-03 08:56:37.921216: seg_0795, shape torch.Size([1, 121, 128, 128]), rank 0\n",
      "2025-10-03 08:56:38.234691: predicting seg_0802\n",
      "2025-10-03 08:56:38.266666: seg_0802, shape torch.Size([1, 93, 128, 128]), rank 0\n",
      "2025-10-03 08:56:38.577622: predicting seg_0804\n",
      "2025-10-03 08:56:38.610785: seg_0804, shape torch.Size([1, 124, 128, 128]), rank 0\n",
      "2025-10-03 08:56:38.921976: predicting seg_0821\n",
      "2025-10-03 08:56:38.961796: seg_0821, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:39.276861: predicting seg_0826\n",
      "2025-10-03 08:56:39.322950: seg_0826, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:39.633834: predicting seg_0829\n",
      "2025-10-03 08:56:39.668159: seg_0829, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:39.987078: predicting seg_0832\n",
      "2025-10-03 08:56:40.031344: seg_0832, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:40.364949: predicting seg_0833\n",
      "2025-10-03 08:56:40.403941: seg_0833, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:40.721025: predicting seg_0838\n",
      "2025-10-03 08:56:40.758573: seg_0838, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:41.073301: predicting seg_0853\n",
      "2025-10-03 08:56:41.105251: seg_0853, shape torch.Size([1, 87, 128, 118]), rank 0\n",
      "2025-10-03 08:56:41.420849: predicting seg_0854\n",
      "2025-10-03 08:56:41.458358: seg_0854, shape torch.Size([1, 126, 128, 128]), rank 0\n",
      "2025-10-03 08:56:41.804223: predicting seg_0865\n",
      "2025-10-03 08:56:41.869832: seg_0865, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:42.204252: predicting seg_0878\n",
      "2025-10-03 08:56:42.249086: seg_0878, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:42.567604: predicting seg_0883\n",
      "2025-10-03 08:56:42.603349: seg_0883, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:42.916162: predicting seg_0884\n",
      "2025-10-03 08:56:42.954759: seg_0884, shape torch.Size([1, 103, 128, 128]), rank 0\n",
      "2025-10-03 08:56:43.267278: predicting seg_0889\n",
      "2025-10-03 08:56:43.315930: seg_0889, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:43.627392: predicting seg_0890\n",
      "2025-10-03 08:56:43.663283: seg_0890, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:43.976511: predicting seg_0893\n",
      "2025-10-03 08:56:44.009807: seg_0893, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:44.324105: predicting seg_0911\n",
      "2025-10-03 08:56:44.356683: seg_0911, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:44.673148: predicting seg_0912\n",
      "2025-10-03 08:56:44.722234: seg_0912, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:45.036923: predicting seg_0913\n",
      "2025-10-03 08:56:45.075791: seg_0913, shape torch.Size([1, 109, 128, 128]), rank 0\n",
      "2025-10-03 08:56:45.396023: predicting seg_0914\n",
      "2025-10-03 08:56:45.438328: seg_0914, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:45.754189: predicting seg_0915\n",
      "2025-10-03 08:56:45.791144: seg_0915, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:46.109706: predicting seg_0925\n",
      "2025-10-03 08:56:46.143648: seg_0925, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:46.457923: predicting seg_0937\n",
      "2025-10-03 08:56:46.493947: seg_0937, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:46.809415: predicting seg_0940\n",
      "2025-10-03 08:56:46.843959: seg_0940, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:47.158903: predicting seg_0942\n",
      "2025-10-03 08:56:47.193805: seg_0942, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-03 08:56:47.508646: predicting seg_0943\n",
      "2025-10-03 08:56:47.545726: seg_0943, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:47.862180: predicting seg_0954\n",
      "2025-10-03 08:56:47.891634: seg_0954, shape torch.Size([1, 107, 128, 128]), rank 0\n",
      "2025-10-03 08:56:48.203801: predicting seg_0955\n",
      "2025-10-03 08:56:48.238815: seg_0955, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:48.550908: predicting seg_0957\n",
      "2025-10-03 08:56:48.593219: seg_0957, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:48.908876: predicting seg_0961\n",
      "2025-10-03 08:56:48.943000: seg_0961, shape torch.Size([1, 120, 128, 128]), rank 0\n",
      "2025-10-03 08:56:49.256757: predicting seg_0962\n",
      "2025-10-03 08:56:49.288924: seg_0962, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:49.602551: predicting seg_0966\n",
      "2025-10-03 08:56:49.637780: seg_0966, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:49.946785: predicting seg_0967\n",
      "2025-10-03 08:56:49.996451: seg_0967, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:50.308935: predicting seg_0974\n",
      "2025-10-03 08:56:50.347474: seg_0974, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:50.665184: predicting seg_0977\n",
      "2025-10-03 08:56:50.704282: seg_0977, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:51.016876: predicting seg_0979\n",
      "2025-10-03 08:56:51.054023: seg_0979, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:51.372555: predicting seg_0981\n",
      "2025-10-03 08:56:51.408887: seg_0981, shape torch.Size([1, 127, 128, 128]), rank 0\n",
      "2025-10-03 08:56:51.724893: predicting seg_0982\n",
      "2025-10-03 08:56:51.759466: seg_0982, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:56:52.074938: predicting seg_0997\n",
      "2025-10-03 08:56:52.106214: seg_0997, shape torch.Size([1, 128, 128, 128]), rank 0\n",
      "2025-10-03 08:57:08.539640: Validation complete\n",
      "2025-10-03 08:57:08.540122: Mean Validation Dice:  0.9819510619646314\n"
     ]
    }
   ],
   "source": [
    "#注意開始訓練前請記得至Terminal確認自己是否有安裝 GCC/G++ 編譯器(C 編譯器)\n",
    "#若無C編譯器，conda activate進入環境後執行\n",
    "#conda install -y -c conda-forge graphviz\n",
    "#conda install -y -c conda-forge gcc_linux-64 gxx_linux-64\n",
    "#指定環境變數\n",
    "#export CC=$CONDA_PREFIX/bin/x86_64-conda-linux-gnu-gcc\n",
    "#export CXX=$CONDA_PREFIX/bin/x86_64-conda-linux-gnu-g++\n",
    "\n",
    "import os\n",
    "\n",
    "# 設定 conda gcc 路徑\n",
    "os.environ['CC'] = \"/home/sandy0317/.conda/envs/nnunet/bin/x86_64-conda-linux-gnu-gcc\" #改成自己的帳號名稱放置路徑中\n",
    "os.environ['CXX'] = \"/home/sandy0317/.conda/envs/nnunet/bin/x86_64-conda-linux-gnu-g++\" #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "# 設定 nnU-Net 資料夾\n",
    "os.environ['nnUNet_raw'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw' #改成自己的帳號名稱放置路徑中\n",
    "os.environ['nnUNet_preprocessed'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed' #改成自己的帳號名稱放置路徑中\n",
    "os.environ['nnUNet_results'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_results' #改成自己的帳號名稱放置路徑中\n",
    "\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "#!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_train Dataset001 3d_fullres 0 -tr nnUNetTrainer_10epochs\n",
    "!/home/sandy0317/.local/bin/nnUNetv2_train Dataset001 3d_fullres 0 -tr nnUNetTrainer_10epochs --device cuda:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc999760-c288-4fe0-84b8-2bcd27ed8474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 3 cases in the source folder\n",
      "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 3 cases that I would like to predict\n",
      "\n",
      "Predicting sample_1:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.34s/it]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_1\n",
      "\n",
      "Predicting sample_2:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.24it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_2\n",
      "\n",
      "Predicting sample_3:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sample_3\n",
      "推論完成，結果存於： nnUNet_predictions\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 6. 推論 (Inference)\n",
    "# ===========================\n",
    "import os\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "os.environ['nnUNet_raw'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_raw'\n",
    "os.environ['nnUNet_preprocessed'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_preprocessed'\n",
    "os.environ['nnUNet_results'] = '/home/sandy0317/practice_nnUNet/practice_nnunet/nnUNet_results'\n",
    "\n",
    "# 測試影像資料夾\n",
    "input_folder = os.path.join(nnUNet_raw, \"Dataset001\", \"imagesTs\")\n",
    "\n",
    "# 改成相對路徑或家目錄下\n",
    "output_folder = 'nnUNet_predictions'  # 或者 os.path.expanduser('~/nnUNet_predictions')\n",
    "\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "#!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_predict -d 1 -i $input_folder -o $output_folder -tr nnUNetTrainer_10epochs -c 3d_fullres -f 0\n",
    "!/home/sandy0317/.local/bin/nnUNetv2_predict -d 1 -i $input_folder -o $output_folder -tr nnUNetTrainer_10epochs -c 3d_fullres -f 0  --device cuda:2\n",
    "\n",
    "print(\"推論完成，結果存於：\", output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c1ac2-ef6c-4272-8739-aa8864321c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 7. 驗證 (可選)\n",
    "# ===========================\n",
    "#改成自己的帳號名稱放置路徑中\n",
    "#!/home/sandy0317/.conda/envs/nnunet/bin/nnUNetv2_evaluate_folder -ref $nnUNet_raw/Dataset001/labelsTr -pred $output_folder\n",
    "!/home/sandy0317/.local/bin/nnUNetv2_evaluate_folder -ref $nnUNet_raw/Dataset001/labelsTr -pred $output_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c3115-db8b-4387-a92c-e04bd855de43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa752444-d60f-4013-b606-7228d0c3b7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnU-Net)",
   "language": "python",
   "name": "nnunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
